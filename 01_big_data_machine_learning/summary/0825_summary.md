# 08/23 수업 내용 정리

---

## 1. 선형 회귀
### 1.1 단순 선형 회귀
| 내용                           | 설명                                                                 |
|-------------------------------|---------------------------------------------------------------------|
| **기본 매커니즘**              | 데이터 산포를 바탕으로 예측력이 가장 뛰어난 직선<br>($wx + b = f(x)$)을 그려내어 새로운 값 예측 |
| **계산 방법**                  | 학습으로 $w$(기울기), $b$(절편)을 구하는 것이 아니라<br>최소 제곱법(ols, linregress, LinearRegression 등)으로 상대적으로 단순하게 계산 |
| **추후 학습 방법**             | 나중에는 미분을 이용한 학습(경사하강법 등)으로 모델을 만드는 방법도 배움 |
| **결과 해석**                  | 최소 제곱법 함수(statsmodels.formula.api.ols, sklearn.linear_model.LinearRegression, scipy.stats.linregress)는 결과로 표(요약 통계)를 제공하며, 이 표에서 정보를 읽을 줄 알아야 함 |

### 1.2.1 선형 회귀 결과 표 해석
**OLS Regression Results 1** 

| 항목                | 값/예시           | 설명                                                                                   |
|---------------------|-------------------|----------------------------------------------------------------------------------------|
| Dep. Variable       | 만족도            | 종속 변수(예측하고자 하는 값), 여기서는 '만족도'                                      |
| R-squared           | 0.588             | 결정계수, 모델이 데이터를 얼마나 잘 설명하는지(0.588 → 약 58.8% 설명력)                |
| Adj. R-squared      | 0.586             | 수정 결정계수, 변수 개수에 따라 보정된 설명력(0.586)                                   |
| Model               | OLS               | 사용한 회귀 모델(OLS: 최소제곱법)                                                      |
| Method              | Least Squares     | 회귀분석에 사용된 방법(Least Squares: 최소제곱법)                                      |
| F-statistic         | 374.0             | 모델 전체의 유의성 검정 통계량(374.0)                                                  |
| Prob (F-statistic)  | 2.24e-52          | F-통계량의 p -value, 모델이 통계적으로 유의미한지 판단                                 |
| Date                | Mon, 25 Aug 2025  | 분석 실행 날짜                                                                         |
| Time                | 09:54:35          | 분석 실행 시간                                                                         |
| Log-Likelihood      | -207.44           | 로그 우도, 모델의 적합도(값이 클수록 적합도가 높음)                                   |
| No. Observations    | 264               | 데이터 샘플 개수(264개)                                                                |
| AIC                 | 418.9             | 모델 평가 지표(Akaike 정보 기준), 값이 낮을수록 좋은 모델                              |
| Df Residuals        | 262               | 잔차(오차) 자유도(262)                                                                 |
| BIC                 | 426.0             | 모델 평가 지표(Bayesian 정보 기준), 값이 낮을수록 좋은 모델                            |
| Df Model            | 1                 | 모델에 사용된 독립 변수 개수(1개)                                                      |
| Covariance Type     | nonrobust         | 공분산 추정 방식(nonrobust: 일반적 방식)                                               |


### 1.2.2 회귀계수 및 통계량
**OLS Regression Results 2** 
| 항목             | 값        | 설명
|------------------|-----------|--------------------------------------------------------------|
| Intercept           | 0.7789                        | 절편(입력값이 0일 때 만족도 예측값)
| Intercept std err   | 0.124                         | 절편의 표준오차(신뢰도)
| Intercept t         | 6.273                         | 절편의 t-통계량(유의성 검정)
| Intercept P>|t|     | 0.000                         | 절편의 p-value(통계적 유의성)
| Intercept [0.025, 0.975] | 0.534 ~ 1.023            | 절편의 95% 신뢰구간
| 적절성 coef         | 0.7393                        | 독립 변수 '적절성'의 회귀계수(영향력)
| 적절성 std err      | 0.038                         | 적절성의 표준오차(신뢰도)
| 적절성 t            | 19.340                        | 적절성의 t-통계량(유의성 검정)
| 적절성 P>|t|        | 0.000                         | 적절성의 p-value(통계적 유의성)
| 적절성 [0.025, 0.975] | 0.664 ~ 0.815               | 적절성의 95% 신뢰구간


#### 1.2.3 잔차 및 기타 지표 
**OLS Regression Results 3** 
| 항목             | 값        | 설명
|------------------|-----------|--------------------------------------------------------------|
| Omnibus          | 11.674    | 잔차의 정규성 검정 통계량
| Prob(Omnibus)    | 0.003     | Omnibus 검정의 p-value
| Durbin-Watson    | 2.185     | 잔차의 자기상관 검정(2에 가까우면 잔차가 독립적임)
| Jarque-Bera (JB) | 16.003    | 잔차의 정규성 검정 통계량
| Prob(JB)         | 0.000335  | Jarque-Bera 검정의 p-value
| Skew             | -0.328    | 잔차의 왜도(비대칭성)
| Kurtosis         | 4.012     | 잔차의 첨도(뾰족함)
| Cond. No.        | 13.4      | 다중공선성 지표(높으면 변수 간 중복 영향 가능성)

---

### 1.4 선형회귀 표를 통해 확인하는 우연일 가능성

- 같은 결과($y = 2x + 0.5$)를 갖는 두 모델이 있을 때,  
  A는 표준 오차(std err)가 작고, B는 표준 오차가 크다면  
  A는 우연일 가능성이 높고, B는 우연일 가능성이 낮다고 판단한다.
- 즉, 결과 표에서 **std err(표준 오차)**가 작으면 우연일 가능성을 의심해야 하며,  
  표준 오차가 크면 데이터의 변동성이 크기 때문에 실제로 의미 있는 결과일 가능성이 높다.
- 표준 오차는 회귀계수의 신뢰도를 판단하는 핵심 지표이므로,  
  단순히 계수의 값만 보는 것이 아니라 표준 오차와 함께 해석해야  
  모델이 우연에 의한 결과인지, 실제로 의미 있는 결과인지 알 수 있다.

---

### 1.5 t-test

t-test는 두 집단의 평균 차이가 통계적으로 유의미한지 판단하는 검정 방법입니다.  
집단 1과 집단 2의 평균 차이를 표준 오차로 나누어 t-값을 계산합니다.  
이 t-값이 클수록 두 집단의 평균 차이가 크다는 의미이고,  
t-값이 커지면 p-value(유의확률)는 작아집니다.  
즉, p-value가 작다는 것은 두 집단의 평균 차이가 우연이 아니라 실제로 존재할 가능성이 높다는 뜻입니다.

- 만약 집단 1과 집단 2의 평균 차이(기울기)가 없으면, 귀무가설을 채택합니다.  
  (즉, 두 집단의 평균이 같다고 본다)
- 반대로 평균 차이(기울기)가 있으면, 귀무가설을 기각합니다.  
  (즉, 두 집단의 평균이 다르다고 본다)

이러한 점에서 t-test와 선형 회귀는 유사한 개념으로 볼 수 있습니다.  
선형 회귀에서도 각 변수의 계수에 대해 t-검정을 실시하여  
해당 계수가 통계적으로 유의미한지(p-value가 작은지) 확인합니다.

### 1.6 독립 변수가 여러 개인 선형 회귀
| 내용             | 설명                                                                                                 |
|------------------|-----------------------------------------------------------------------------------------------------|
| **기본 매커니즘** | 여러 개의 독립 변수($x_1, x_2, x_3, x_4$ 등)에 각각의 가중치($w_1, w_2, w_3, w_4$)를 곱해 모두 더하고, 절편($b$)을 더해서 종속 변수($y$)를 예측함.<br>즉, 여러 요인이 결과에 동시에 영향을 주는 경우에 사용. <br>공식: $y = w_1x_1 + w_2x_2 + w_3x_3 + w_4x_4 + b$ |
| **계산 방법**    | 최소 제곱법(OLS, LinearRegression 등)을 사용해 각 변수의 회귀계수($w_1, w_2, ...$)와 절편($b$)을 계산.<br>데이터가 많아질수록 변수 간의 상관관계와 각 변수의 영향력을 더 정확하게 파악할 수 있음. |
| **결과 해석**    | 각 독립 변수의 회귀계수는 해당 변수의 영향력(기여도)을 의미.<br>회귀 결과 표에서 각 변수의 계수, p-value, 결정계수(R²) 등을 확인하여 모델의 적합도와 변수의 유의성을 해석함.<br>변수가 많을수록 다중공선성(변수 간 중복 영향)도 함께 고려해야 함. |
| **추가 사항**    | 다중 선형 회귀는 변수 선택, 변수 변환, 정규화 등 추가적인 데이터 전처리와 모델 평가가 중요함.<br>실제 현업에서는 변수 간의 관계를 시각화하거나, 변수 선택 기법을 활용해 모델의 성능을 높임. |

### 2. 선형회귀 분석의 기본 이해(https://cafe.daum.net/flowlife/SBU0/29)
* 알아두기 : sklearn의 LinearRegression()은 인공신경망과는 달리 랜덤한 w로 시작하지 않는다.
대신, 수학적으로 해석적인 해 (closed-form solution)을 구한다. 즉, 정확한 w를 수식으로 바로 계산한다. 이건 최적화가 아니라 수식으로 바로 계산하는 방식이다. 따라서 랜덤한 w를 시작점으로 쓰는 게 아니라, 한 번에 정답을 계산하는 구조다.

### 2.1 결정계수(R²) 상세 설명

결정계수(R², Coefficient of Determination)는 회귀분석에서 모델이 데이터를 얼마나 잘 설명하는지 나타내는 지표입니다.  
값의 범위는 0~1 사이이며, 1에 가까울수록 모델의 설명력이 높다는 뜻입니다.

---

👌 **결정계수 해석 기준**

결정계수 R² 값은 "좋다 / 나쁘다"를 딱 잘라 말하기 어렵다. 데이터 특성과 분석 목적에 따라 기준이 달라지기 때문이다.

다만 실무에서 자주 참고하는 경험적 기준은 아래와 같다.

- **실무에서의 R² 해석 기준 (일반적인 기준)**
    - 0.0 ~ 0.3 : 설명력이 매우 낮음 → 모델 개선 필요
    - 0.3 ~ 0.5 : 어느 정도 경향 설명 가능 (탐색적 분석, 사회과학 데이터에서는 종종 수용)
    - 0.5 ~ 0.7 : 중간 수준의 설명력, 실무에서 "쓸 만하다"고 보는 경우 있음
    - 0.7 ~ 0.9 : 높은 설명력, 예측 모델로서 꽤 신뢰할 수 있음
    - 0.9 이상 : 매우 높은 설명력 (단, 과적합 가능성 주의)

- **분야별 차이**
    - 사회과학, 심리학, 교육학 : 사람 행동은 변수가 많아 R²=0.3 만 나와도 "꽤 설명력 있다"고 평가한다.
    - 자연과학, 공학, 물리학 : 실험 데이터가 상대적으로 정밀 → R²=0.8 이상을 기대하는 경우 많다.
    - 비즈니스 / 마케팅 : 매출, 고객 행동 예측은 노이즈가 커서 R²=0.5 정도만 나와도 실무에서 의미 있다고 본다.
    - 딥러닝/머신러닝 회귀 모델 : 데이터 크고 변수가 많으면 R² 단일 값보다 RMSE, MAE 같은 오차 지표도 함께 본다.

- **결론**
    - "최소 몇 이상"이라는 절대 기준은 없음
    - 사회과학 → 0.3 이상, 비즈니스 → 0.5 이상, 공학/자연과학 → 0.7~0.8 이상 정도면 실무에서 수용되는 경우가 많다.
    - 그런데 R² 값 하나만 보지 말고, 오차 지표(RMSE, MAE) + 시각화(잔차분석) 까지 같이 평가하는 게 바람직하다.

---

#### 공식

- $R^2 = 1 - \dfrac{SSE}{SST}$

여기서 각 항목의 의미는 다음과 같습니다.

| 항목 | 의미 | 수식 |
|------|------|------|
| SST (Sum of Squares Total) | 전체 데이터의 변동량(실제값과 평균값의 차이의 제곱합) | $\sum (y_i - \bar{y})^2$ |
| SSR (Sum of Squares Regression) | 모델이 설명한 변동량(예측값과 평균값의 차이의 제곱합) | $\sum (\hat{y}_i - \bar{y})^2$ |
| SSE (Sum of Squares Error) | 모델이 설명하지 못한 오차(실제값과 예측값의 차이의 제곱합) | $\sum (y_i - \hat{y}_i)^2$ |

#### 해석

- **SST**: 전체 데이터의 총 변동량. 데이터의 평균을 기준으로 각 데이터가 얼마나 퍼져 있는지 나타냄.
- **SSR**: 모델이 예측한 값이 평균에서 얼마나 벗어나 있는지, 즉 모델이 설명한 변동량.
- **SSE**: 모델의 예측이 실제값과 얼마나 차이가 나는지, 즉 모델이 설명하지 못한 오차.

#### 결정계수 계산

- $R^2 = 1 - \dfrac{SSE}{SST}$  
  → 모델의 오차(SSE)가 전체 변동량(SST)에 비해 얼마나 작은지로 설명력 판단

- $R^2 = \dfrac{SSR}{SST}$  
  → 모델이 전체 변동량 중 얼마나 많은 부분을 설명했는지로 판단

#### 예시

- $R^2 = 0$ : 모델이 데이터를 전혀 설명하지 못함 (예측이 평균과 같음)
- $R^2 = 1$ : 모델이 데이터를 완벽하게 설명함 (예측이 실제값과 모두 같음)
- $0 < R^2 < 1$ : 모델이 어느 정도 데이터를 설명함

#### 참고

- MSE(Mean Squared Error)도 오차를 평가하는 지표로 자주 사용됨
- 결정계수는 회귀분석의 모델 평가에서 가장 기본적이고 직관적인 지표임
- 참조 : 회귀와 검정 - 통계적인 해석 t, F검정
https://recipesds.tistory.com/entry/%ED%9A%8C%EA%B7%80%EC%99%80-%EA%B2%80%EC%A0%95-%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D%EA%B2%B0%EA%B3%BC%EA%B0%80-%EC%9C%A0%EC%9D%98%ED%95%9C%EA%B0%80-%ED%86%B5%EA%B3%84%EC%A0%81%EC%9D%B8-%ED%95%B4%EC%84%9D-t-F%EA%B2%80%EC%A0%95-%EA%B7%B8%EB%A6%AC%EA%B3%A0-%EA%B0%91%EB%B6%84%EC%8B%B8-ANOVA-%EC%97%A5

### 수정된 결정계수(Adjusted R²)란?

수정된 결정계수(Adjusted R²)는 일반 결정계수(R²)의 단점을 보완한 지표입니다.  
R²는 독립 변수(설명 변수)가 많아질수록 값이 무조건 높아지는 경향이 있습니다.  
즉, 쓸모없는 변수를 추가해도 R²가 올라가서 모델이 실제로 더 좋아졌는지 알기 어렵습니다.

**수정된 R²(Adjusted R²)는 변수의 개수를 고려하여 보정한 결정계수**입니다.  
불필요한 변수를 추가하면 오히려 값이 낮아질 수 있으므로,  
모델의 진짜 설명력을 평가할 때 더 신뢰할 수 있습니다.

#### 공식

$$
\text{Adjusted } R^2 = 1 - \left( \frac{(1 - R^2) \times (n - 1)}{n - p - 1} \right)
$$

- $n$ : 샘플 개수
- $p$ : 독립 변수(설명 변수) 개수

#### 요약

- **R²** : 모델이 데이터를 얼마나 잘 설명하는지(설명력)
- **Adjusted R²** : 변수 개수를 고려해 실제 설명력을 평가(불필요한 변수 추가 시 패널티 부여)
- 모델 비교, 변수 선택 등에서 Adjusted R²가 더 신뢰도 높은 지표로 사용됨

---

## 3. 좋은 선형회귀 : 상관 관계가 높은 데이터를 활용(lm6.py 참고)

| 변수         | coef   | std err | t      | P>|t|  | [0.025   | 0.975] |
|--------------|--------|---------|--------|------|----------|--------|
| Intercept    | 1.8560 | 0.251   | 7.401  | 0.000| 1.360    | 2.352  |
| petal_length | 0.7091 | 0.057   | 12.502 | 0.000| 0.597    | 0.821  |
| petal_width  | -0.5565| 0.128   | -4.363 | 0.000| -0.809   | -0.304 |
| sepal_width  | 0.6508 | 0.067   | 9.765  | 0.000| 0.519    | 0.783  |

### 다중 선형회귀 예측식 설명

다중 선형회귀에서는 여러 개의 독립 변수와 각각의 회귀계수(coef)를 사용하여 종속 변수(y)를 예측합니다.
위 표는 sepal_length를 종속변수, petal_length, petal_width, sepal_width를 독립변수로 설정한 표입니다.

**예측식 구조:**

$$
y = \text{Intercept} + (\text{petal\_length} \times x_1) + (\text{petal\_width} \times x_2) + (\text{sepal\_width} \times x_3)
$$

- **Intercept**: 절편(모든 입력값이 0일 때 y의 예측값)
- **petal_length, petal_width, sepal_width**: 각 독립 변수의 회귀계수(coef)
- **$x_1, x_2, x_3$**: 각각의 입력값(예: 꽃잎 길이, 꽃잎 너비, 꽃받침 너비 등)

**예시**
- Intercept = 1.8560
- petal_length = 0.7091
- petal_width = -0.5565
- sepal_width = 0.6508

입력값이 각각 $x_1, x_2, x_3$라면,

$$
y = 1.8560 + 0.7091 \times x_1 - 0.5565 \times x_2 + 0.6508 \times x_3
$$

**주의:**  
모든 coef에 같은 x를 곱하는 것이 아니라,  
각 변수에 해당하는 입력값을 곱해서 더한 뒤 Intercept(절편)를 더해야 합니다.

---

## 4. 종속 변수와 독립 변수의 관계

선형회귀 분석에서는 **종속 변수**(예측하고자 하는 값)와 **독립 변수**(설명 변수, 입력값) 간의 관계를 수식으로 표현합니다.

- **종속 변수**는 모델이 예측하는 대상입니다. 예를 들어, 위 표에서는 `sepal_length`가 종속 변수입니다.
- **독립 변수**는 종속 변수에 영향을 주는 요인들로, 각각의 변수마다 회귀계수(coef)가 존재합니다. 위 표에서는 `petal_length`, `petal_width`, `sepal_width`가 독립 변수입니다.

회귀분석 결과 표에서 각 독립 변수의 coef(회귀계수)는  
해당 변수의 값이 1 증가할 때 종속 변수(y)가 얼마나 변하는지를 의미합니다.

**예시:**  
- `petal_length`의 coef가 0.7091이면, petal_length가 1 증가할 때 sepal_length는 0.7091만큼 증가합니다.
- `petal_width`의 coef가 -0.5565이면, petal_width가 1 증가할 때 sepal_length는 0.5565만큼 감소합니다.
- `sepal_width`의 coef가 0.6508이면, sepal_width가 1 증가할 때 sepal_length는 0.6508만큼 증가합니다.

**정리:**  
- 종속 변수는 여러 독립 변수의 영향을 받아 결정됩니다.
- 각 독립 변수의 영향력(기여도)은 회귀계수(coef)로 나타나며,  
  이 값이 양수면 양의 영향, 음수면 음의 영향을 의미합니다.
- 절편(Intercept)은 모든 독립 변수 값이 0일 때 종속 변수의 예측값입니다.

---

## 5. 귀납과 연역

| 용어            | 설명                                                                                   |
|-----------------|----------------------------------------------------------------------------------------|
| **귀납**        | 여러 개의 개별적인 사례나 관찰을 바탕으로 일반적인 원리나 법칙을 도출하는 사고 방식.<br>예: 여러 번 실험해서 모두 같은 결과가 나오면, "이 현상은 항상 그렇다"고 일반화함. |
| **연역**        | 이미 알려진 일반적인 원리나 법칙을 바탕으로, 개별적인 사례나 결과를 예측하는 사고 방식.<br>예: "모든 사람은 죽는다"라는 법칙이 있으면, "소크라테스도 사람이다 → 소크라테스도 죽는다"라고 결론을 내림. |
| **귀납법적 추론** | 구체적인 사실이나 데이터를 관찰하여, 그로부터 일반적인 결론이나 법칙을 도출하는 과정.<br>예: 여러 학생의 시험 결과를 보고 "공부를 많이 하면 성적이 오른다"는 결론을 내림. |
| **연역법적 추론** | 일반적인 원리나 법칙을 전제로 하여, 그로부터 구체적인 사실이나 결과를 예측하는 과정.<br>예: "공부를 많이 하면 성적이 오른다"는 법칙이 있으면, "철수가 공부를 많이 했으니 성적이 올랐을 것이다"라고 예측함. |

참고: 
- 베이지안 이론(연역법 + 귀납법), 사전 데이터를 갖고 사전 검증을 하고 사후 검증을 진행
- 머신러닝은 귀납법적 추론이다.

---

## 6. mtcars 데이터 셋

`mtcars` 데이터셋은 자동차의 다양한 특성을 담고 있는 유명한 예제 데이터입니다.  
주로 회귀분석, 상관분석, 데이터 시각화 등 통계 및 머신러닝 실습에 많이 사용됩니다.

| 변수명   | 설명                                   |
|----------|----------------------------------------|
| mpg      | 연비(Miles/(US) gallon)                |
| cyl      | 실린더 개수                            |
| disp     | 배기량(Displacement, cubic inches)     |
| hp       | 마력(Horsepower)                       |
| drat     | 후륜 기어비(Ratio)                     |
| wt       | 차량 무게(Weight, 1000 lbs)            |
| qsec     | 1/4마일 주파 시간(Seconds)             |
| vs       | 엔진 형태(V-shaped/Straight)           |
| am       | 변속기 종류(Automatic/Manual)           |
| gear     | 기어 수                                |
| carb     | 카뷰레터 수                            |

**활용 예시**
- mpg(연비)를 종속 변수로 두고, 나머지 변수들을 독립 변수로 회귀분석을 할 수 있음
- 변수 간 상관관계 분석, 시각화, 변수 선택 등 다양한 실습에 활용

---

## 7. 회귀분석과 관련된 정보 이해(https://cafe.daum.net/flowlife/SBU0/42)
*** 선형회귀분석의 기존 가정 충족 조건 ***

. 선형성 : 독립변수(feature)의 변화에 따라 종속변수도 일정 크기로 변화해야 한다.

. 정규성 : 잔차항(오차항)이 정규분포를 따라야 한다.

. 독립성 : 독립변수의 값이 서로 관련되지 않아야 한다.(여성 데이터를 이용하여 유전 정보를 조사 -> 여성 데이터의 여성이 서로 가족이면 안 됌)

. 등분산성 : 그룹간의 분산이 유사해야 한다. 독립변수의 모든 값에 대한 오차들의 분산은 일정해야 한다.

. 다중공선성 : 다중회귀 분석 시 두 개 이상의 독립변수 간에 강한 상관관계가 있어서는 안된다.(독립 변수끼리)

---

## 8. 잔차 간 자기 상관
잔차(residual)란 회귀분석에서 실제값과 예측값의 차이(오차)를 의미합니다.  
잔차 간 자기 상관(residual autocorrelation)은 이 오차들이 서로 연관되어 있는지를 나타내는 개념입니다.

### 왜 중요한가?
- **잔차가 서로 독립적이어야** 선형회귀의 기본 가정이 충족됩니다.
- 만약 잔차에 자기 상관이 있으면(즉, 이전 오차가 다음 오차에 영향을 주면),  
  모델이 데이터의 패턴을 제대로 설명하지 못하고 있다는 신호입니다.
- 자기 상관이 있으면 회귀계수의 신뢰도가 떨어지고,  
  예측 결과가 왜곡될 수 있습니다.

### 대표적 검정 방법: Durbin-Watson 통계량
- 회귀분석 결과표에서 **Durbin-Watson 값**을 확인합니다.
- 값의 해석:
    - **2에 가까우면** 잔차가 서로 독립적임(문제 없음)
    - **2보다 많이 작으면**(예: 1 이하) 양의 자기 상관(오차가 비슷하게 반복됨)
    - **2보다 많이 크면**(예: 3 이상) 음의 자기 상관(오차가 번갈아 반복됨)
- 일반적으로 **1.5~2.5 사이**면 큰 문제 없다고 봅니다.

### 예시
- Durbin-Watson = 2.185 → 잔차가 독립적임(문제 없음)
- Durbin-Watson = 1.1 → 잔차에 양의 자기 상관 있음(모델 개선 필요)

### 해결 방법
- 자기 상관이 심하면,  
  - 변수 추가/변환,  
  - 시계열 모델(ARIMA 등) 사용,  
  - 잔차 분석 및 모델 재설계가 필요합니다.

**요약:**  
잔차 간 자기 상관은 회귀분석의 신뢰도를 좌우하는 중요한 요소입니다.  
Durbin-Watson 등 지표를 꼭 확인하고, 자기 상관이 있으면 모델을 개선해야 합니다.

---

## 8. 잔차 간 자기 상관

잔차(residual)란 회귀분석에서 실제값과 예측값의 차이(오차)를 의미합니다.  
잔차 간 자기 상관(residual autocorrelation)은 이 오차들이 서로 연관되어 있는지를 나타내는 개념입니다.

### 왜 중요한가?
- **잔차가 서로 독립적이어야** 선형회귀의 기본 가정이 충족됩니다.
- 만약 잔차에 자기 상관이 있으면(즉, 이전 오차가 다음 오차에 영향을 주면),  
  모델이 데이터의 패턴을 제대로 설명하지 못하고 있다는 신호입니다.
- 자기 상관이 있으면 회귀계수의 신뢰도가 떨어지고,  
  예측 결과가 왜곡될 수 있습니다.

### 대표적 검정 방법: Durbin-Watson 통계량
- 회귀분석 결과표에서 **Durbin-Watson 값**을 확인합니다.
- 값의 해석:
    - **2에 가까우면** 잔차가 서로 독립적임(문제 없음)
    - **2보다 많이 작으면**(예: 1 이하) 양의 자기 상관(오차가 비슷하게 반복됨)
    - **2보다 많이 크면**(예: 3 이상) 음의 자기 상관(오차가 번갈아 반복됨)
- 일반적으로 **1.5~2.5 사이**면 큰 문제 없다고 봅니다.

### 예시
- Durbin-Watson = 2.185 → 잔차가 독립적임(문제 없음)
- Durbin-Watson = 1.1 → 잔차에 양의 자기 상관 있음(모델 개선 필요)

### 해결 방법
- 자기 상관이 심하면,  
  - 변수 추가/변환,  
  - 시계열 모델(ARIMA 등) 사용,  
  - 잔차 분석 및 모델 재설계가 필요합니다.

---

## 9. 등분산성과 자기 공산성

### 등분산성(Homoscedasticity)
등분산성이란 회귀분석에서 잔차(오차)의 분산이 모든 독립 변수 값에 대해 일정해야 한다는 가정입니다.  
즉, 예측값이 크든 작든 잔차의 크기가 일정하게 분포해야 합니다.

- **등분산성이 깨지면(이분산성, Heteroscedasticity)**  
  - 잔차의 분산이 특정 구간에서 커지거나 작아집니다.
  - 회귀계수의 신뢰도가 떨어지고, p-value 등 통계적 검정 결과가 왜곡될 수 있습니다.
  - 잔차 vs 예측값 그래프에서 잔차가 한쪽으로 퍼지거나, 특정 패턴이 보이면 이분산성을 의심합니다.

- **해결 방법**
  - 변수 변환(로그, 제곱근 등)
  - 가중 회귀(Weighted Least Squares)
  - robust 옵션 사용 등

### 자기 공산성(Multicollinearity)
자기 공산성(다중공선성)은 다중회귀분석에서 두 개 이상의 독립 변수들이 서로 강하게 상관관계를 가질 때 발생합니다.

- **문제점**
  - 독립 변수들이 서로 영향을 주면, 각 변수의 회귀계수(coef)가 불안정해집니다.
  - 변수의 중요도를 정확히 해석하기 어렵고, 모델의 예측력이 떨어질 수 있습니다.
  - VIF(Variance Inflation Factor) 등 지표로 진단합니다. VIF 값이 10 이상이면 다중공선성을 의심합니다.

- **해결 방법**
  - 상관관계가 높은 변수 제거
  - 변수 선택 기법(Feature Selection)
  - PCA 등 차원 축소 방법 활용

---

## 기타 전달사항
- 금요일 끝날 때쯤 팀 편성 및 자리 변경, 다음 주 월요일부터 바뀐 자리에서 수업 진행
- 파이널 프로젝트 진행할 때는 오전에 수업하고 오후에는 프로젝트 시간을 비워줍니다.
- 표준오차와 R^2 꼭 기억하세요
- 전통적인 선형회귀에서는 학습이 아닌, 최소 제곱법을 통해 모델을 만든다.
- 팀이 편성되면 스토리가 있는 문제를 내면 팀 단위로 문제를 풀이합니다.