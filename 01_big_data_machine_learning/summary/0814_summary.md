# 통계 분석 요약 및 상세 설명

---

## 1. ANOVA(분산분석)란?
- 여러 집단의 평균 차이가 통계적으로 유의한지 검정하는 방법
- 집단 간 분산(집단 평균의 차이)과 집단 내 분산(집단 내부의 변동)을 비교하여 평균 차이가 있는지 판단
- **일원 ANOVA**: 한 기준(예: 성별)으로 집단을 구분하여 평균 차이 검정  
- **이원 ANOVA**: 두 기준(예: 성별, 지역)으로 집단을 구분하여 평균 차이 검정
- **가정**: 정규성(데이터가 정규분포를 따른다), 등분산성(집단 간 분산이 같다), 독립성(관측값이 서로 독립적이다)
- **해석**: p값 < 0.05이면 집단 간 평균 차이가 있다고 판단  
- 평균 차이가 발견되면, 어떤 집단 간에 차이가 있는지 확인하기 위해 사후검정(post-hoc test)을 실시

### 반복측정 ANOVA(Repeated Measures ANOVA)
- 동일한 집단(또는 개인)에서 여러 시점 또는 조건에서 반복적으로 측정한 데이터의 평균 차이를 검정
- 예시: 한 집단의 혈압을 약 복용 전, 복용 후, 1주일 후 등 여러 번 측정

### 이원 반복측정 ANOVA(Two-Way Repeated Measures ANOVA)
- 두 가지 요인(예: 시간, 약 종류 등)에 대해 반복 측정된 데이터의 평균 차이와 상호작용 효과를 검정
- 예시: 여러 시간대와 여러 약 종류를 모두 반복 측정한 경우, 시간과 약 종류의 효과 및 상호작용을 분석

---

## 2. 가설 검정의 기본 구조

### 귀무가설(Null Hypothesis, H₀)
- "차이가 없다", "효과가 없다", "관계가 없다" 등 연구자가 검정하고자 하는 현상에 대해 부정적인 입장을 취하는 가설
- 예: 두 집단의 평균이 같다, 두 변수 사이에 상관관계가 없다

### 대립가설(Alternative Hypothesis, H₁)
- "차이가 있다", "효과가 있다", "관계가 있다" 등 연구자가 주장하고자 하는 현상에 대해 긍정적인 입장을 취하는 가설
- 예: 두 집단의 평균이 다르다, 두 변수 사이에 상관관계가 있다

### 검정 통계량과 p-value
- 검정 통계량: 데이터로부터 계산된 값(예: t값, F값, 카이제곱값 등)
- p-value: 귀무가설이 참일 때, 관측된 데이터보다 더 극단적인 결과가 나올 확률  
  - p-value < 0.05: 통계적으로 유의함 → 귀무가설 기각, 대립가설 채택  
  - p-value ≥ 0.05: 통계적으로 유의하지 않음 → 귀무가설 채택

#### 예시: 평균수명 차이 검정
- **귀무가설(H₀):** 개와 고양이의 평균수명은 서로 같다.  
  (μ_개 = μ_고양이)
- **대립가설(H₁):** 개와 고양이의 평균수명은 서로 다르다.  
  (μ_개 ≠ μ_고양이)
- 검정 방법: t-test(두 집단 평균 비교), 또는 ANOVA(세 집단 이상 평균 비교)

---

## 3. 카이제곱 검정(Chi-squared test) 개념

### 종류
- **적합도 검정(Goodness of Fit)**: 한 집단의 관측 빈도가 기대 빈도와 일치하는지 검정
- **동질성 검정(Homogeneity)**: 여러 집단의 분포가 동일한지 검정
- **독립성 검정(Independence)**: 두 범주형 변수 간에 관계가 있는지 검정

### 절차
1. 교차분할표(빈도표) 작성
2. 기대값 계산: (행합 × 열합) / 전체합
3. 카이제곱 통계량 계산: Σ((관측값 - 기대값)² / 기대값)
4. 자유도(df) 계산: (행의 수 - 1) × (열의 수 - 1)
5. 임계값(critical value) 또는 p-value로 유의성 판단

### 해석
- 검정 통계량이 임계값보다 크거나, p-value가 0.05 미만이면 귀무가설 기각(관계 있음)
- 그렇지 않으면 귀무가설 채택(관계 없음)

#### 예시 가설
- **귀무가설(H₀):** 벼락치기 공부와 합격 여부는 관계가 없다.
- **대립가설(H₁):** 벼락치기 공부와 합격 여부는 관계가 있다.

#### 판정 기준
- **카이제곱 통계량 < 임계값** 또는 **p-value > 0.05** → 귀무가설 채택(관계 없음)
- **카이제곱 통계량 ≥ 임계값** 또는 **p-value ≤ 0.05** → 귀무가설 기각(관계 있음)

---

## 4. 실제 코드 예시 및 해석
```python
import pandas as pd
import scipy.stats as stats

data = pd.read_csv("./01_big_data_machine_learning/data/pass_cross.csv")
ctab = pd.crosstab(index=data['공부안함'], columns=data['불합격'], margins=True)
chi2, p, dof, expected = stats.chi2_contingency(ctab)
print(chi2, p, dof, expected)
```
- 예시 결과:  
  - 카이제곱 통계량(chi2) = 3.0  
  - 임계값(cv) = 3.84 (자유도 1, 유의수준 0.05 기준)  
  - p-value = 0.5578
- 해석:  
  - **3.0 < 3.84** 또는 **0.5578 > 0.05**  
  - 귀무가설 채택 → 벼락치기 공부와 합격 여부는 통계적으로 유의한 관계가 없음

---

## 5. p-value란?
- p-value는 귀무가설이 참일 때, 관측된 통계량보다 더 극단적인 값이 나올 확률
- `scipy.stats.chi2_contingency()` 함수가 자동으로 계산해줌
- p-value가 작을수록(0.05 미만) 귀무가설을 기각할 근거가 강해짐

---

## 6. 참고
- 카이제곱 검정은 표본 데이터가 우연히 발생한 것인지, 실제로 변수 간에 관계가 있는지 판단하는 데 사용
- 교차분석은 범주형 변수 간의 관계를 시각적으로 파악하고, 통계적으로 검증하는 데 매우 유용

---

### 참고 링크
- [Chi-squared test - Wikipedia](https://en.wikipedia.org/wiki/Chi-squared_test)
- [Scipy.stats.chi2_contingency - SciPy Documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2_contingency.html)


좋은 질문입니다!

카이제곱 검정에서 cv(임계값)과 p-value는 같은 의미를 가집니다.

cv(임계값): 자유도와 유의수준(예: 0.05)에 따라 카이제곱 분포표에서 찾은 값입니다.
→ 검정 통계량이 이 값보다 크면 귀무가설을 기각합니다.

p-value: 실제 검정 통계량이 나올 확률을 직접 계산한 값입니다.
→ p-value가 유의수준(예: 0.05)보다 작으면 귀무가설을 기각합니다.

즉, 두 방법은 본질적으로 같은 기준을 사용합니다.

임계값 방식: "검정 통계량 > 임계값" → 기각
p-value 방식: "p-value < 유의수준" → 기각
p-value는 임계값을 직접 찾지 않아도 되므로 더 직관적이고 편리하게 사용됩니다.

그래서 실무와 논문에서는 p-value 기준이 더 많이 쓰입니다.


카이제곱 검정에서 p-value 계산 과정
카이제곱 통계량(χ²) 계산

각 셀에 대해:
(관측값 - 기대값)² / 기대값
모든 셀에 대해 합산
예시:
χ² = Σ((Oᵢ - Eᵢ)² / Eᵢ)
(Oᵢ: 관측값, Eᵢ: 기대값)
자유도(df) 결정

df = (행의 수 - 1) × (열의 수 - 1)
카이제곱 분포에서 p-value 계산

계산된 χ² 값이 **카이제곱 분포(자유도 df)**에서
**오른쪽 꼬리(χ² 이상)**에 있을 확률을 구함
즉,
p-value = P(카이제곱 분포(df) ≥ 계산된 χ² 값)
수식 예시 (Python 없이)
예를 들어, χ² = 14.2, 자유도 = 5라면
p-value = ∫₁₄.₂^∞ [카이제곱 분포(5)]의 확률밀도함수 dx

이 확률은 통계분포표나 적분으로 구함
(실제로는 소프트웨어가 계산)

요약:
p-value는 "카이제곱 분포에서 계산된 χ² 값보다 더 큰 값이 나올 확률"을 의미하며,
이 확률이 작으면(0.05 미만) 귀무가설을 기각합니다.

trade가 작아질 수록 statics는 커진다.