# Gradient Exploding (기울기 폭주)

Gradient Exploding(기울기 폭주)는 딥러닝 모델의 학습 과정에서 역전파(backpropagation) 시, 네트워크의 가중치가 지나치게 큰 값으로 업데이트되어 학습이 불안정해지는 현상입니다.

## 원인
- **깊은 신경망 구조**: 레이어가 많아질수록, 역전파 시 기울기가 반복적으로 곱해지면서 값이 기하급수적으로 커질 수 있습니다.
- **가중치 초기값이 너무 큼**: 초기 가중치가 크면, 역전파 시 기울기가 폭주할 가능성이 높아집니다.
- **적절하지 않은 활성화 함수**: ReLU 등 일부 활성화 함수는 입력값이 클 때 기울기가 커질 수 있습니다.

## 증상
- 학습 중 손실(loss)이 NaN 또는 무한대(inf)로 치솟음
- 가중치 값이 비정상적으로 커짐
- 모델이 제대로 수렴하지 않음

## 수학적 설명
역전파 과정에서 각 레이어의 기울기는 이전 레이어의 기울기와 가중치의 곱으로 전달됩니다. 만약 이 곱이 1보다 크면, 레이어가 깊어질수록 기울기가 기하급수적으로 커집니다.

$$
\frac{\partial L}{\partial w^{(l)}} = \frac{\partial L}{\partial w^{(L)}} \prod_{k=l}^{L-1} w^{(k)}
$$

## 해결 방법
1. **가중치 초기화 방법 개선**: Xavier, He 초기화 등 적절한 초기화 기법 사용
2. **Gradient Clipping**: 기울기의 크기를 일정 값 이상으로 제한하여 폭주 방지
3. **적절한 학습률 설정**: 너무 큰 learning rate는 폭주를 유발할 수 있으므로, 작은 값부터 시도
4. **정규화 기법 적용**: Batch Normalization 등으로 각 레이어의 분포를 안정화

## 참고
- Gradient Exploding은 Gradient Vanishing(기울기 소실)과 함께 딥러닝에서 대표적인 학습 장애 요인입니다.