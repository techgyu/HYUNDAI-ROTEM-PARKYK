# Linear Regression vs Logistic Regression

## 1. 공통점
- 두 모델 모두 **wx + b**(입력의 선형 결합)를 기반으로 추세선을 만듦
- 입력 변수(독립변수)는 연속형 데이터를 사용

## 2. 차이점
- **Linear Regression**
    - 목적: **연속형 값**(예: 집값, 온도 등) 예측
    - 출력: 실수값(예: 3.5, 100.2 등)
    - 종속변수(y): 연속형
    - 예측값이 실제값과 얼마나 가까운지 MSE(평균제곱오차) 등으로 평가

- **Logistic Regression**
    - 목적: **이진 분류**(예: 스팸/비스팸, 질병 유/무 등)
    - 출력: 0~1 사이의 **확률값**
    - 종속변수(y): 범주형(0 또는 1)
    - 예측값을 **sigmoid 함수**로 0~1 사이로 변환  
      \[
      \text{sigmoid}(z) = \frac{1}{1 + e^{-z}}
      \]
    - 0.5 기준으로 0/1로 분류
    - 다항 분류(클래스가 3개 이상)에서는 **softmax 함수** 사용

## 3. Logistic Regression의 구체적 설명
- **확률과 가능도(우도, likelihood)**
    - wx+b로 계산된 값을 sigmoid 함수로 확률로 변환
    - 여러 sigmoid(wx+b) 중에서 가장 잘 맞는(최적의) wx+b를 찾는 것이 중요
    - 이를 위해 **최대 우도법(MLE, Maximum Likelihood Estimation)**을 사용
        - 실제 데이터가 관측될 확률(가능도)을 최대화하는 파라미터(wx, b)를 찾음

- **신경망과의 연결**
    - Logistic Regression은 **퍼셉트론**(단일 뉴런)과 구조적으로 동일
    - 딥러닝의 기본 단위가 됨
    - 케라스 등 딥러닝 프레임워크에서 활성화 함수로 sigmoid, softmax, relu 등을 사용

## 4. 딥러닝과의 연관성
- Logistic Regression을 이해하면 인공신경망, 딥러닝의 기본 원리를 쉽게 이해할 수 있음
- 신경망의 각 뉴런은 wx+b를 계산하고, 활성화 함수(sigmoid, softmax 등)로 출력값을 변환

---

**구체화가 필요한 부분**
- **최대 우도법(MLE)**:  
  실제 데이터가 주어졌을 때, 모델이 그 데이터를 생성할 확률(가능도)을 최대화하는 파라미터를 찾는 방법.  
  Logistic Regression에서는 이 방법으로 wx+b(가중치와 절편)를 최적화함.

- **sigmoid vs softmax**:  
  - sigmoid: 이진 분류(0/1)에서 사용, S자 곡선  
  - softmax: 다항 분류(3개 이상 클래스)에서 사용, 각 클래스별 확률을 합이 1이 되도록 변환

- **딥러닝 연결**:  
  Logistic Regression의 구조가 퍼셉트론과 동일하며,  
  여러 퍼셉트론이 모여 인공신경망을 구성함.

---

## 5. 다중 회귀 공식 정리

- **다중 선형 회귀**  
    ### f(x) = w₁x₁ + w₂x₂ + ... + wₙxₙ + b

- **다중 로지스틱 회귀(이진 분류)**  
    ### f(x) = exp(wx + b) / (1 + exp(wx + b))

- **다중 로지스틱 회귀(다항 분류, softmax)**  
    ### P(y = k | x) = exp(wₖx + bₖ) / Σ_j exp(wⱼx + bⱼ)

---

## 6. 트랜스포머 기반 대형 언어 모델의 결과 생성 원리

ChatGPT나 Gemini와 같은 트랜스포머 기반 대형 언어 모델(LLM)은 최근 자연어 처리 분야에서 혁신적인 성능을 보여주고 있습니다.  
이 모델들은 수십억~수천억 개의 파라미터를 가진 딥러닝 신경망으로, 방대한 텍스트 데이터를 학습하여 인간과 유사한 언어 생성, 추론, 요약, 번역 등 다양한 작업을 수행합니다.

하지만 이러한 모델이 **어떻게 복잡한 언어적 맥락을 이해하고, 창의적이고 논리적인 답변을 생성하는지**에 대한  
정확한 내부 원리와 메커니즘은 아직 명확히 밝혀지지 않았습니다.

- 트랜스포머의 핵심 구조(Attention, Self-Attention 등)는 공개되어 있고, 각 층이 입력의 어떤 부분에 집중하는지 시각화할 수 있지만,  
  왜 이런 구조가 인간 수준의 언어 능력과 추론, 창의성까지 가능하게 하는지에 대한 **이론적 설명과 수학적 해석**은 아직 부족합니다.

- 각 파라미터와 뉴런이 실제로 어떤 역할을 하는지, 모델이 어떻게 맥락을 장기적으로 기억하고, 복잡한 질문에 논리적으로 답변하는지에 대한  
  **설명 가능한 인공지능(XAI)** 연구가 활발히 진행 중이지만, 아직 완전한 해석은 이루어지지 않았습니다.

- 현재로서는 "많은 데이터와 큰 모델, 그리고 트랜스포머 구조"가 놀라운 결과를 만들어낸다는 경험적 사실만 확인되고 있으며,  
  그 내부 동작 원리는 "블랙박스"에 가까운 상태입니다.

## 7. 로지스틱 회귀분석(Logistic Regression Analysis) - 통계기초.pdf | 69 page
- 0.5를 기준으로 0과 1로 이항 분류한다.
- 합격/불합격, 성공/실패, 생존/사망, 진실/거짓 등 이분법적인 결과를 도출하기 위해 주로 사용되는 회귀
분석 방식으로 예측을 주목적으로 하는 회귀분석과 차이가 있다. 
- 여러 개의 S자가 생기는데 이 중 무엇이 Best인지 판단하는 최대 우도 법을 사용하여 판단한다.

## 8. softmax 함수
- softmax 함수는 다항 분류에서 사용되는 활성화 함수로, 각 클래스의 확률을 계산하여 출력값의 합이 1이 되도록 변환한다.
- 주어진 입력 벡터 \( z \)에 대해 softmax 함수는 다음과 같이 정의된다:
\[
\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}
\]
- 여기서 \( z_i \)는 클래스 \( i \)에 대한 로짓(logit) 값이며, \( e \)는 자연상수이다.
- softmax 함수는 각 클래스의 상대적인 확률을 제공하므로, 다중 클래스 분류 문제에서 유용하게 사용된다.
  ### 실습 파일: logi3.py

## 9. 혼동 행렬(Confusion Matrix)
-  TP |  FP
-  FN |  TN

TP: True Positive (참 긍정)
FP: False Positive (거짓 긍정)
FN: False Negative (거짓 부정)
TN: True Negative (참 부정)

## 10. Data Scaling을 하는 이유
- 데이터의 스케일(범위)이 다르면 모델 학습에 영향을 미칠 수 있다.
- 특히 경사 하강법(Gradient Descent) 기반의 알고리즘에서는 각 특성의 스케일이 비슷해야 수렴 속도가 빨라진다.
- 표준화(Standardization) 또는 정규화(Normalization)를 통해 데이터의 스케일을 맞춰준다.

## 롤 베이스와 데이터 베이스(지도 학습)의 차이
- 롤 베이스: 주로 비정형 데이터(예: 텍스트, 이미지 등)에 대한 학습에 사용
- 데이터 베이스: 정형 데이터(예: 표 형식의 데이터)에 대한 학습에 사용