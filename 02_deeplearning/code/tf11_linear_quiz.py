import tensorflow as tf # AI ëª¨ë¸ ë§Œë“œëŠ” ë©”ì¸ ë„êµ¬
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Input # ë‰´ëŸ´ë„¤íŠ¸ì›Œí¬  ì¸µë“¤
from tensorflow.keras import optimizers # ëª¨ë¸ê³¼ ìµœì í™”ê¸°
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler # ì„±ëŠ¥ ì¸¡ì • ë„êµ¬
from tensorflow.keras.layers import Input
from tensorflow.keras import Model

# ë¬¸ì œ1) ì•„ë²„ì§€ í‚¤ë¡œ ì•„ë“¤ í‚¤ ì˜ˆì¸¡í•˜ëŠ” íšŒê·€ë¶„ì„ ëª¨ë¸
# https://cafe.daum.net/flowlife/S2Ul/25 
# dataë¥¼ ì´ìš©í•´ ì•„ë²„ì§€ í‚¤ë¡œ ì•„ë“¤ì˜ í‚¤ë¥¼ ì˜ˆì¸¡í•˜ëŠ” íšŒê·€ë¶„ì„ ëª¨ë¸ì„ ì‘ì„±í•˜ì‹œì˜¤.
#  - train / test ë¶„ë¦¬
#  - Sequential apiì™€ function api ë¥¼ ì‚¬ìš©í•´ ëª¨ë¸ì„ ë§Œë“¤ì–´ ë³´ì‹œì˜¤.
#  - trainê³¼ testì˜ mseë¥¼ ì‹œê°í™” í•˜ì‹œì˜¤
#  - ìƒˆë¡œìš´ ì•„ë²„ì§€ í‚¤ì— ëŒ€í•œ ìë£Œë¡œ ì•„ë“¤ì˜ í‚¤ë¥¼ ì˜ˆì¸¡í•˜ì‹œì˜¤.

# 1. ë°ì´í„° ë¡œë“œ
url = "https://github.com/data-8/materials-fa17/raw/master/lec/galton.csv"

data = pd.read_csv(url)
# print(data.head())
# print(f"ë°ì´í„° í¬ê¸°: {data.shape}")
# print(f"ì»¬ëŸ¼: {data.columns}")

# 2. ë°ì´í„° ì „ì²˜ë¦¬
# ë‚¨ì„± ìë…€(ì•„ë“¤)ë§Œ í•„í„°ë§
male_children = data[data['gender'] == 'male']
# print(f"ì „ì²´ ë°ì´í„°: {len(data)}, ë‚¨ì„± ìë…€: {len(male_children)}")

X = male_children['father'].values.reshape(-1, 1)     # ì•„ë²„ì§€ í‚¤ (ì…ë ¥)
y = male_children['childHeight'].values.reshape(-1, 1) # ì•„ë“¤ í‚¤ (ì¶œë ¥)

# print(f"ì…ë ¥ ë°ì´í„° í˜•íƒœ: {X.shape}")
# print(f"ì¶œë ¥ ë°ì´í„° í˜•íƒœ: {y.shape}")

# 3. Train/Test ë¶„ë¦¬
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
# print(f"í›ˆë ¨ ë°ì´í„°: {X_train.shape}, í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape}")

# 4. ë°ì´í„° ì‹œê°í™”
plt.figure(figsize=(10, 6))
plt.scatter(X_train, y_train, alpha=0.6, label='Train Data')
plt.scatter(X_test, y_test, alpha=0.6, label='Test Data', color='green')
plt.xlabel('Father Height (inches)')
plt.ylabel('Son Height (inches)')
plt.title('Galton Data: Father vs Son Height')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show() 

# AI ëª¨ë¸ 2ê°€ì§€ ìƒì„±
# Sequential APIì™€ Functional API ëª¨ë¸ ìƒì„±

# 5. Sequential API ëª¨ë¸
print("\nğŸ¤– Sequential API ëª¨ë¸ ìƒì„±")
model_seq = Sequential([
    Dense(16, activation='relu'),  # ì¸µ1: 16ê°œ ë‰´ëŸ°
    Dense(8, activation='relu'),   # ì¸µ2: 8ê°œ ë‰´ëŸ°  
    Dense(1, activation='linear')  # ì¸µ3: 1ê°œ ë‰´ëŸ° (í‚¤ ì˜ˆì¸¡)
])
model_seq.compile(
    optimizer=optimizers.Adam(learning_rate=0.01),
    loss='mse',
    metrics=['mae']
)

# 6. Functional API ëª¨ë¸
print("\nğŸ”§ Functional API ëª¨ë¸ ìƒì„±")


inputs = Input(shape=(1,))
x = Dense(16, activation='relu')(inputs)
x = Dense(8, activation='relu')(x)
outputs = Dense(1, activation='linear')(x)

model_func = Model(inputs=inputs, outputs=outputs)
model_func.compile(
    optimizer=optimizers.Adam(learning_rate=0.01),
    loss='mse',
    metrics=['mae']
)

# 7. ë‘ ëª¨ë¸ í•™ìŠµ
print("\nğŸ”¥ Sequential ëª¨ë¸ í•™ìŠµ...")
# validation_split=0.2: í›ˆë ¨ ë°ì´í„°ì˜ 20%ë¥¼ ê²€ì¦ìš©ìœ¼ë¡œ ì‚¬ìš©
history_seq = model_seq.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0)

print("ğŸ”¥ Functional ëª¨ë¸ í•™ìŠµ...")
history_func = model_func.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0)

# 8. Trainê³¼ Test MSE ì‹œê°í™”
plt.figure(figsize=(15, 5))

# Sequential ëª¨ë¸ ê²°ê³¼
plt.subplot(1, 3, 1)
plt.plot(history_seq.history['loss'], label='Train Loss')
plt.plot(history_seq.history['val_loss'], label='Val Loss')
plt.title('Sequential API - Loss')
plt.xlabel('Epoch')
plt.ylabel('MSE')
plt.legend()
plt.grid(True, alpha=0.3)

# Functional ëª¨ë¸ ê²°ê³¼
# loss ê·¸ë˜í”„: í•™ìŠµì´ ì§„í–‰ë ìˆ˜ë¡ ì˜¤ì°¨ê°€ ì¤„ì–´ë“œëŠ” ëª¨ìŠµ
plt.subplot(1, 3, 2)
plt.plot(history_func.history['loss'], label='Train Loss')
plt.plot(history_func.history['val_loss'], label='Val Loss')
plt.title('Functional API - Loss')
plt.xlabel('Epoch')
plt.ylabel('MSE')
plt.legend()
plt.grid(True, alpha=0.3)

# ì˜ˆì¸¡ ê²°ê³¼ ë¹„êµ
plt.subplot(1, 3, 3)
y_pred_seq = model_seq.predict(X_test)
y_pred_func = model_func.predict(X_test)

plt.scatter(y_test, y_pred_seq, alpha=0.6, label='Sequential', s=30)
plt.scatter(y_test, y_pred_func, alpha=0.6, label='Functional', s=30)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
plt.xlabel('real height')
plt.ylabel('predicted height')
plt.title('Predicted vs Real')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# 9. ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
seq_r2 = r2_score(y_test, y_pred_seq)
func_r2 = r2_score(y_test, y_pred_func)

print(f"\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ:")
print(f"Sequential API RÂ² ìŠ¤ì½”ì–´: {seq_r2:.4f}")
print(f"Functional API RÂ² ìŠ¤ì½”ì–´: {func_r2:.4f}")

# 10. ìƒˆë¡œìš´ ì•„ë²„ì§€ í‚¤ë¡œ ì•„ë“¤ í‚¤ ì˜ˆì¸¡
print("\nğŸ¯ ìƒˆë¡œìš´ ì•„ë²„ì§€ í‚¤ë¡œ ì•„ë“¤ í‚¤ ì˜ˆì¸¡:")
new_father_heights = np.array([70, 72, 68, 75]).reshape(-1, 1)

pred_seq = model_seq.predict(new_father_heights)
pred_func = model_func.predict(new_father_heights)

for i, father_height in enumerate(new_father_heights.flatten()):
    print(f"ì•„ë²„ì§€ í‚¤ {father_height} inches:")
    print(f"  Sequential ì˜ˆì¸¡: {pred_seq[i][0]:.2f} inches")
    print(f"  Functional ì˜ˆì¸¡: {pred_func[i][0]:.2f} inches")

print("\nâœ… ê°¤íŠ¼ ë°ì´í„° ë¶„ì„ ì™„ë£Œ!")

# ë¬¸ì œ2) ìì „ê±° ê³µìœ  ì‹œìŠ¤í…œ ë¶„ì„ - ë‹¤ì¤‘ì„ í˜•íšŒê·€ë¶„ì„
# https://raw.githubusercontent.com/pykwon/python/refs/heads/master/data/train.csv
# ìì „ê±° ê³µìœ  ì‹œìŠ¤í…œ ë¶„ì„ìš© ë°ì´í„° train.csvë¥¼ ì´ìš©í•˜ì—¬ ëŒ€ì—¬íšŸìˆ˜ì— ì˜í–¥ì„ ì£¼ëŠ” ë³€ìˆ˜ë“¤ì„ ê³¨ë¼ ë‹¤ì¤‘ì„ í˜•íšŒê·€ë¶„ì„ ëª¨ë¸ì„ ì‘ì„±í•˜ì‹œì˜¤.
# ëª¨ë¸ í•™ìŠµì‹œì— ë°œìƒí•˜ëŠ” lossë¥¼ ì‹œê°í™”í•˜ê³  ì„¤ëª…ë ¥ì„ ì¶œë ¥í•˜ì‹œì˜¤.
# ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ input í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ í‚¤ë³´ë“œë¡œ ì…ë ¥í•˜ì—¬ ëŒ€ì—¬íšŸìˆ˜ ì˜ˆì¸¡ê²°ê³¼ë¥¼ ì½˜ì†”ë¡œ ì¶œë ¥í•˜ì‹œì˜¤.

# 1. ë°ì´í„° ë¡œë“œ
print("ğŸš´â€â™‚ï¸ ìì „ê±° ê³µìœ  ì‹œìŠ¤í…œ ë°ì´í„° ë¶„ì„")
url = "https://raw.githubusercontent.com/pykwon/python/refs/heads/master/data/train.csv"

bike_data = pd.read_csv(url)
# print(f"ë°ì´í„° í¬ê¸°: {bike_data.shape}")
# print(f"ì»¬ëŸ¼: {bike_data.columns}")
# print(bike_data.head())

# 2. ë°ì´í„° íƒìƒ‰ ë° ì „ì²˜ë¦¬
print("\nğŸ“Š ë°ì´í„° ê¸°ë³¸ ì •ë³´:")
print(bike_data.describe())

# ìƒê´€ê´€ê³„ ë¶„ì„
# ìƒê´€ê´€ê³„: ì–´ë–¤ ë³€ìˆ˜ê°€ ëŒ€ì—¬íšŸìˆ˜ì— ê°€ì¥ í° ì˜í–¥ì„ ì£¼ëŠ”ì§€ ë¶„ì„
# íŠ¹ì„± ì„ íƒ: ì˜¨ë„, ì²´ê°ì˜¨ë„, ìŠµë„, í’ì†, ê³„ì ˆ, ë‚ ì”¨, ê·¼ë¬´ì¼

# ìˆ«ì ì»¬ëŸ¼ë§Œ ì„ íƒí•´ì„œ ìƒê´€ê´€ê³„ ê³„ì‚° (ë‚ ì§œ/ì‹œê°„ ì»¬ëŸ¼ ì œì™¸)
numeric_columns = bike_data.select_dtypes(include=[np.number])
correlation = numeric_columns.corr()

print("\nğŸ” ëŒ€ì—¬íšŸìˆ˜(count)ì™€ ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ì˜ ìƒê´€ê´€ê³„:")
if 'count' in correlation.columns:
    count_corr = correlation['count'].sort_values(ascending=False)
    print(count_corr)
else:
    print("âš ï¸ 'count' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ìˆ«ì ì»¬ëŸ¼ë“¤:")
    print(numeric_columns.columns.tolist())
    # ê¸°ë³¸ íŠ¹ì„± ì‚¬ìš©
    available_features = [col for col in ['temp', 'atemp', 'humidity', 'windspeed', 'season', 'weather', 'workingday'] 
                         if col in numeric_columns.columns]
    print(f"ì‚¬ìš© ê°€ëŠ¥í•œ íŠ¹ì„±: {available_features}")

# ëŒ€ì—¬íšŸìˆ˜ì— ì˜í–¥ì„ ì£¼ëŠ” ì£¼ìš” ë³€ìˆ˜ ì„ íƒ (ìƒê´€ê³„ìˆ˜ ì ˆëŒ“ê°’ ê¸°ì¤€)
# ì‹¤ì œ ë°ì´í„°ì— ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ
all_possible_features = ['temp', 'atemp', 'humidity', 'windspeed', 'season', 'weather', 'workingday', 'casual', 'registered']
important_features = [col for col in all_possible_features if col in bike_data.columns]

# count ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ëŒ€ì²´ íƒ€ê²Ÿ ì°¾ê¸°
target_col = 'count'
if 'count' not in bike_data.columns:
    possible_targets = ['cnt', 'total', 'demand']
    for col in possible_targets:
        if col in bike_data.columns:
            target_col = col
            break
    if target_col == 'count':  # ì—¬ì „íˆ ì°¾ì§€ ëª»í–ˆë‹¤ë©´
        # ìˆ«ì ì»¬ëŸ¼ ì¤‘ ë§ˆì§€ë§‰ ì»¬ëŸ¼ì„ íƒ€ê²Ÿìœ¼ë¡œ ì‚¬ìš©
        numeric_cols = bike_data.select_dtypes(include=[np.number]).columns.tolist()
        target_col = numeric_cols[-1] if numeric_cols else 'count'

print(f"\nğŸ¯ ì„ íƒëœ íŠ¹ì„±: {important_features}")
print(f"ğŸ¯ íƒ€ê²Ÿ ë³€ìˆ˜: {target_col}")

# íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
X = bike_data[important_features]
y = bike_data[target_col]

print(f"ì…ë ¥ íŠ¹ì„± í˜•íƒœ: {X.shape}")
print(f"íƒ€ê²Ÿ í˜•íƒœ: {y.shape}")

# 3. ë°ì´í„° ì •ê·œí™”
# ì •ê·œí™”: ëª¨ë“  ë³€ìˆ˜ë¥¼ ê°™ì€ ìŠ¤ì¼€ì¼ë¡œ ë§ì¶”ê¸°
# ì´ìœ : ì˜¨ë„(0-41)ì™€ ìŠµë„(0-100)ì˜ ë‹¨ìœ„ê°€ ë‹¤ë¥´ë‹ˆê¹Œ
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train/Test ë¶„ë¦¬
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)
print(f"í›ˆë ¨ ë°ì´í„°: {X_train.shape}, í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape}")

# 4. Sequential API ëª¨ë¸ ìƒì„±

print("\nğŸ¤– Sequential API ëª¨ë¸ ìƒì„±...")
# ë‹¤ì¤‘ ì…ë ¥: 7ê°œ ë³€ìˆ˜(ì˜¨ë„, ìŠµë„, ê³„ì ˆ ë“±)ë¥¼ ë™ì‹œì— ê³ ë ¤
# ë³µì¡í•œ ëª¨ë¸: ì—¬ëŸ¬ ì¸µìœ¼ë¡œ ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ
model_seq = Sequential([
    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    Dense(32, activation='relu'),
    Dense(16, activation='relu'),
    Dense(1, activation='linear')  # íšŒê·€ì´ë¯€ë¡œ linear
])

model_seq.compile(
    optimizer=optimizers.Adam(learning_rate=0.001),
    loss='mse',
    metrics=['mae']
)

# 5. ëª¨ë¸ í•™ìŠµ (history ì €ì¥)
print("ğŸ”¥ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
history = model_seq.fit(
    X_train, y_train,
    epochs=100,
    batch_size=32,
    validation_split=0.2,
    verbose=1
)

# 6. Loss ì‹œê°í™”
# 2ê°œ ê·¸ë˜í”„: Loss(ì˜¤ì°¨) ë³€í™”, MAE(í‰ê· ì ˆëŒ€ì˜¤ì°¨) ë³€í™”
# ê³¼ì í•© í™•ì¸: í›ˆë ¨/ê²€ì¦ ì†ì‹¤ì´ ê°™ì´ ë–¨ì–´ì§€ëŠ”ì§€ í™•ì¸

plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss During Training')
plt.xlabel('Epoch')
plt.ylabel('Loss (MSE)')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(history.history['mae'], label='Training MAE')
plt.plot(history.history['val_mae'], label='Validation MAE')
plt.title('Model MAE During Training')
plt.xlabel('Epoch')
plt.ylabel('Mean Absolute Error')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# 7. ëª¨ë¸ í‰ê°€
train_pred = model_seq.predict(X_train)
test_pred = model_seq.predict(X_test)

train_r2 = r2_score(y_train, train_pred)
test_r2 = r2_score(y_test, test_pred)

print(f"\nğŸ“ˆ ëª¨ë¸ ì„±ëŠ¥:")
print(f"í›ˆë ¨ ë°ì´í„° RÂ² ìŠ¤ì½”ì–´: {train_r2:.4f}")
print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° RÂ² ìŠ¤ì½”ì–´: {test_r2:.4f}")
print(f"í›ˆë ¨ ë°ì´í„° ì„¤ëª…ë ¥: {train_r2*100:.2f}%")
print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„¤ëª…ë ¥: {test_r2*100:.2f}%")

# 8. ìƒˆë¡œìš´ ë°ì´í„° ì˜ˆì¸¡ (í‚¤ë³´ë“œ ì…ë ¥)
print("\nğŸ® ìƒˆë¡œìš´ ë°ì´í„°ë¡œ ëŒ€ì—¬íšŸìˆ˜ ì˜ˆì¸¡í•˜ê¸°!")
print("ë‹¤ìŒ ì •ë³´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”:")

# ì‹¤ì œ ì‚¬ìš©ëœ íŠ¹ì„± ê°œìˆ˜ë§Œí¼ ì…ë ¥ë°›ê¸°
print(f"ğŸ“ ì…ë ¥í•  íŠ¹ì„±: {important_features}")
print(f"ğŸ“Š ì´ {len(important_features)}ê°œ íŠ¹ì„±")

new_values = []
feature_descriptions = {
    'temp': 'ì˜¨ë„ (0-41)',
    'atemp': 'ì²´ê°ì˜¨ë„ (0-50)', 
    'humidity': 'ìŠµë„ (0-100)',
    'windspeed': 'í’ì† (0-67)',
    'season': 'ê³„ì ˆ (1:ë´„, 2:ì—¬ë¦„, 3:ê°€ì„, 4:ê²¨ìš¸)',
    'weather': 'ë‚ ì”¨ (1:ë§‘ìŒ, 2:íë¦¼, 3:ë¹„/ëˆˆ)',
    'workingday': 'ê·¼ë¬´ì¼ ì—¬ë¶€ (0:ì•„ë‹ˆì˜¤, 1:ì˜ˆ)',
    'casual': 'ë¹„íšŒì› ëŒ€ì—¬ìˆ˜ (ì˜ˆ: 50)',
    'registered': 'íšŒì› ëŒ€ì—¬ìˆ˜ (ì˜ˆ: 200)',
    'holiday': 'íœ´ì¼ ì—¬ë¶€ (0:ì•„ë‹ˆì˜¤, 1:ì˜ˆ)'
}

for feature in important_features:
    description = feature_descriptions.get(feature, f'{feature} ê°’')
    if feature in ['season', 'weather', 'workingday', 'holiday']:
        value = int(input(f"{description}: "))
    else:
        value = float(input(f"{description}: "))
    new_values.append(value)

# ìƒˆ ë°ì´í„° ì „ì²˜ë¦¬ (ì •í™•í•œ íŠ¹ì„± ê°œìˆ˜ë¡œ)
new_data = np.array([new_values])
print(f"ğŸ” ì…ë ¥ëœ ë°ì´í„° í˜•íƒœ: {new_data.shape}")
print(f"ğŸ” ì˜ˆìƒë˜ëŠ” íŠ¹ì„± ê°œìˆ˜: {len(important_features)}")

new_data_scaled = scaler.transform(new_data)

# ì˜ˆì¸¡
prediction = model_seq.predict(new_data_scaled)

print(f"\nğŸ¯ ì˜ˆì¸¡ ê²°ê³¼:")
print(f"ì˜ˆìƒ ìì „ê±° ëŒ€ì—¬íšŸìˆ˜: {prediction[0][0]:.0f} ëŒ€")
    
print("\nâœ… ìì „ê±° ê³µìœ  ì‹œìŠ¤í…œ ë¶„ì„ ì™„ë£Œ!")