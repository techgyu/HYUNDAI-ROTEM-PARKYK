# 1. 최근접 이웃(K-NN, K-Nearest Neighbors)

최근접 이웃(K-NN)은 새로운 데이터가 주어졌을 때, 기존 데이터 중에서 가장 가까운 K개의 이웃을 찾아 다수결(분류) 또는 평균(회귀)으로 결과를 예측하는 지도 학습 알고리즘입니다.  
거리 기반으로 동작하며, 별도의 학습 과정 없이 저장된 데이터를 활용해 예측합니다.

## 주요 특징
- **비모수 모델**: 데이터 분포에 대한 가정 없이 동작
- **거리 계산**: 유클리드 거리, 맨해튼 거리 등 다양한 거리 척도 사용 가능
- **K값 선택**: K가 작으면 노이즈에 민감, 크면 경계가 흐려짐
- **메모리 기반**: 모든 학습 데이터를 저장하고 예측 시 활용

## 장점
- 구현이 간단하고 직관적임
- 다양한 문제(분류/회귀)에 적용 가능
- 데이터가 충분하면 높은 성능 가능

## 단점
- 데이터가 많으면 예측 속도가 느림
- 고차원 데이터(차원의 저주)에 취약
- 특성 스케일에 민감(정규화 필요)

## 사용 예시 (scikit-learn)
```python
from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)

y_pred = knn.predict(X_test)
```

## 참고 자료
- [scikit-learn KNN 문서](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)
- [K-NN 위키백과](https://ko.wikipedia.org/wiki/%EC%B5%9C%EA%B7%BC%EC%A0%91_%EC%9D%B4%EC%9A%94)

# 2. 인공신경망(Artificial Neural Networks, ANN)

인공신경망(ANN)은 인간의 뇌 구조와 신경세포(뉴런)의 연결 방식을 모방하여 만든 기계 학습 모델입니다. 여러 개의 뉴런이 층(layer) 형태로 연결되어 입력 데이터를 처리하고, 복잡한 패턴이나 비선형 관계를 학습할 수 있습니다.

## 주요 특징
- **다층 구조**: 입력층, 은닉층(hidden layer), 출력층으로 구성
- **비선형 변환**: 활성화 함수(ReLU, Sigmoid 등)를 통해 비선형 문제 해결 가능
- **가중치 학습**: 각 연결(뉴런 간)에 가중치가 존재하며, 학습 과정에서 최적화됨
- **역전파(Backpropagation)**: 오차를 출력에서 입력 방향으로 전파하며 가중치를 조정

## 장점
- 복잡하고 비선형적인 데이터도 효과적으로 학습 가능
- 이미지, 음성, 자연어 등 다양한 분야에 적용 가능
- 딥러닝(Deep Learning)의 기반이 되는 모델

## 단점
- 많은 데이터와 계산 자원이 필요함
- 과적합(overfitting) 위험이 있음
- 모델 해석이 어려움(블랙박스)

## 사용 예시 (scikit-learn MLPClassifier)
```python
from sklearn.neural_network import MLPClassifier

mlp = MLPClassifier(hidden_layer_sizes=(100, ), activation='relu', max_iter=300, random_state=42)
mlp.fit(X_train, y_train)
y_pred = mlp.predict(X_test)
```

## 참고 자료
- [scikit-learn MLP 문서](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)
- [인공신경망 위키백과](https://ko.wikipedia.org/wiki/%EC%9D%B8%EA%B3%B5_%EC%8B%A0%EA%B2%BD%EB%A7%9D)

# 3. 중심차분법(Central Difference Method, CDM)은 수치 미분 기법 중 하나로, 함수의 도함수를 근사적으로 계산하는 방법입니다. 중심차분법은 주어진 점을 기준으로 양쪽에 있는 두 점의 함수 값을 사용하여 도함수를 추정합니다.

# 4. 단층 퍼셉트론과 다층 퍼셉트론

**단층 퍼셉트론(Single Layer Perceptron)**은 입력층과 출력층만으로 구성된 가장 기본적인 인공신경망 구조입니다. 입력값에 가중치를 곱해 모두 더한 뒤, 활성화 함수를 거쳐 출력값을 만듭니다.  
단층 퍼셉트론은 선형적으로 분리 가능한 문제만 해결할 수 있습니다. 즉, XOR 문제와 같이 비선형적인 데이터는 학습할 수 없습니다.

**다층 퍼셉트론(Multi Layer Perceptron, MLP)**은 입력층, 하나 이상의 은닉층(hidden layer), 그리고 출력층으로 구성됩니다. 각 층의 뉴런들은 이전 층의 모든 뉴런과 연결되어 있으며, 은닉층과 활성화 함수 덕분에 비선형적인 복잡한 문제도 해결할 수 있습니다.  
다층 퍼셉트론은 역전파(Backpropagation) 알고리즘을 통해 가중치를 학습하며, 이미지·음성·자연어 등 다양한 분야에서 활용됩니다.

## 비교
- **단층 퍼셉트론**: 구조가 단순, 선형 분류만 가능
- **다층 퍼셉트론**: 은닉층 추가로 비선형 문제 해결, 복잡한 패턴 학습 가능

# 5. 활성화 함수

활성화 함수(Activation Function)는 인공신경망의 각 뉴런에서 입력 신호의 총합을 출력 신호로 변환하는 역할을 합니다.  
비선형성을 도입하여 신경망이 복잡한 패턴을 학습할 수 있게 해줍니다.

## 주요 활성화 함수
- **시그모이드(Sigmoid)**: 출력값을 0~1 사이로 변환. 이진 분류에 사용.  
  `f(x) = 1 / (1 + exp(-x))`
- **하이퍼볼릭 탄젠트(Tanh)**: 출력값을 -1~1 사이로 변환.  
  `f(x) = tanh(x)`
- **ReLU(Rectified Linear Unit)**: 0 이하 입력은 0, 0 초과 입력은 그대로 출력.  
  `f(x) = max(0, x)`  
  학습 속도가 빠르고, 딥러닝에서 가장 널리 사용됨.
- **Softmax**: 다중 클래스 분류의 출력층에서 사용. 각 클래스의 확률로 변환.

## 역할
- 신경망에 비선형성을 부여하여 복잡한 데이터 구조 학습 가능
- 출력값의 범위를 조정하여 다음 층으로 전달

# 6. rule base 보다 딥러닝이 더 좋은 이유

Rule base(규칙 기반) 시스템은 사람이 직접 규칙을 설계하여 문제를 해결합니다. 하지만 복잡한 패턴이나 예외가 많은 실제 데이터에서는 모든 경우의 규칙을 만들기 어렵고, 유지보수도 힘듭니다.

딥러닝은 데이터를 기반으로 스스로 복잡한 패턴과 특징을 학습할 수 있습니다.  
특히 이미지, 음성, 자연어처럼 규칙으로 표현하기 어려운 문제에서 높은 성능을 보입니다.

## 비교
- **Rule base**: 사람이 직접 규칙을 설계, 단순·명확한 문제에 적합, 예외 처리 어려움
- **딥러닝**: 데이터에서 패턴을 자동으로 학습, 복잡·비정형 데이터에 강함, 예외 상황도 일부 대응 가능

## 딥러닝의 장점
- 대규모 데이터에서 복잡한 관계와 패턴을 자동으로 추출
- 규칙으로 표현하기 어려운 문제(예: 얼굴 인식, 음성 인식, 번역 등)에 강력함
- 새로운 데이터나 환경 변화에도 적응 가능

## 한계
- 많은 데이터와 계산 자원이 필요
- 결과 해석이 어려움(블랙박스)
- 데이터 품질에 따라 성능 좌우

따라서, 단순하고 규칙이 명확한 문제는 rule base가 효율적일 수 있지만,  
복잡하고 다양한 패턴이 존재하는 문제에서는 딥러닝이 더 좋은 성능을 보입니다.

# 7. 역전파

역전파(Backpropagation)는 인공신경망에서 가중치를 학습하는 핵심 알고리즘입니다.  
신경망의 출력과 실제 값(정답) 사이의 오차를 계산한 뒤, 이 오차를 네트워크의 각 층을 거슬러 올라가며(역방향으로) 각 가중치가 오차에 얼마나 영향을 미쳤는지 계산합니다.  
이 정보를 바탕으로 가중치를 조금씩 수정하여 신경망이 더 정확한 예측을 하도록 만듭니다.

## 동작 원리
1. **순전파(Forward Propagation)**  
   입력 데이터를 신경망에 넣어 각 층을 거쳐 출력값을 계산합니다.
2. **오차 계산**  
   출력값과 실제 값(정답) 사이의 오차(손실 함수)를 계산합니다.
3. **역전파(Backward Propagation)**  
   오차를 출력층에서 입력층 방향으로 전달하며, 각 가중치에 대한 오차의 기울기(미분값)를 계산합니다.
4. **가중치 업데이트**  
   계산된 기울기를 이용해 경사하강법(Gradient Descent) 등 최적화 알고리즘으로 가중치를 조정합니다.

## 특징
- 다층 퍼셉트론(MLP) 등 딥러닝 모델의 학습에 필수적
- 오차가 각 층의 가중치에 어떻게 영향을 주는지 수치적으로 계산
- 효율적으로 대규모 신경망을 학습시킬 수 있음

## 한계
- 매우 깊은 신경망에서는 기울기 소실/폭주 문제가 발생할 수 있음
- 활성화 함수와 초기화 방법, 최적화 기법 선택이 중요

## 참고 자료

# 8. 에포크(Epoch)

에포크(Epoch)는 인공신경망 등 머신러닝 모델을 학습할 때 전체 학습 데이터를 한 번 모두 사용하는 과정을 의미합니다.  
즉, 한 에포크는 모델이 모든 학습 데이터를 한 번씩 학습한 것을 뜻합니다.

## 상세 설명
- 학습 과정에서 데이터셋을 여러 번 반복해서 모델에 입력하는데, 이 반복 횟수를 에포크라고 부릅니다.
- 에포크가 1이면 모든 데이터를 한 번 학습한 것이고, 에포크가 10이면 전체 데이터를 10번 반복해서 학습한 것입니다.
- 에포크 수가 너무 적으면 모델이 충분히 학습하지 못하고, 너무 많으면 과적합(overfitting)이 발생할 수 있습니다.

## 관련 용어
- **배치(Batch)**: 전체 데이터 중 일부만을 한 번에 모델에 입력하여 학습하는 단위
- **스텝(Step)**: 한 배치가 모델에 입력되어 가중치가 업데이트되는 과정
- **미니배치(Mini-batch)**: 전체 데이터를 작은 단위로 나누어 학습하는 방식

## 예시
```python
from sklearn.neural_network import MLPClassifier

mlp = MLPClassifier(max_iter=100)  # max_iter가 에포크 수 역할
mlp.fit(X_train, y_train)
```

## 참고
- 적절한 에포크 수는 검증 데이터의 성능을 기준으로 조절
- 조기 종료(Early Stopping) 기법을 사용해 과적합을 방지할 수