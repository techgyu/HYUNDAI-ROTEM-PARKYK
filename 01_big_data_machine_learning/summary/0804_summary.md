# 08/04 수업 + 코딩 주요 개념 요약

---

## 1. 팀 구성 및 운영 원칙

### 1-1. 팀 구성 방식
- 2팀으로 나누어 진행
- 구성원들의 의견을 최대한 반영하여 팀 구성
- 빠른 시간 안에 팀을 꾸려 활동 시작

### 1-2. 팀장 선발 기준
- 실력보다는 리더십을 중시
- 깔끔한하게 일 잘하는 사람 선호
- 기준을 충족하면 누구나 선발 가능
- 팀장이 정해지는 즉시 팀 구성

### 1-3. 팀 운영 방침
- 조직적으로 움직일 것
- 마음 맞는 친구끼리 모이면 팀을 쪼갤 수 있음
- 양팀이 균형을 이루며, 도울 것은 돕고 경쟁할 것은 경쟁 (상호경쟁)
- 부지런히 활동할 것
- 컴퓨터공학과 학생들이 밀리면 안 됨

---

## 2. 학습 및 과제 진행 방식

### 2-1. 과제 제출 방식
- 앞으로는 답을 제공하지 않음
- 각자 직접 문제를 풀어서 제출
- 제출 후 함께 검토 및 토론

### 2-2. 학습 방법론
- 한 가지 방법에만 의존하지 말 것
- 여러 명이 푼 다양한 해결방법을 함께 검토
- 다양한 접근법을 통한 학습 효과 극대화
- 원리를 이해하며 공부할 것

---

## 3. 평가 및 프로젝트 안내

### 3-1. 평가 안내
- 다음주 월요일 평가 예정
- 시험 범위: 넘파이, 판다스, matplotlib (총 15문제)
- 문제 구성: 10문제(5점), 5문제(10점)
- 합격 기준: 60점 이상
- 빈 칸 제출 금지
- 성적과 출석은 기록으로 남음

### 3-2. 프로젝트 안내
- 중간 프로젝트 진행 예정
- 면접에서 활용할 수 있는 포트폴리오 제작 목표

---

## 4. 기술적 학습 내용

### 4-1. 넘파이 사용 원칙
- 앞으로 모든 연산은 넘파이의 함수를 사용

---

## 5. 로그(log)와 스케일링

### 5-1. 로그(log) 변환
- **사용 목적:**  
  데이터 값 간의 편차(스케일 차이)가 매우 클 때, 데이터 분포를 완화하여 분석이나 모델링의 성능을 높이기 위해 사용한다.  
  예를 들어, 일부 값이 지나치게 크거나 작은 경우, 로그 변환을 통해 값의 범위를 줄이고 이상치의 영향을 완화할 수 있다.

- **스케일링이란:**  
  서로 다른 범위(스케일)를 가진 데이터를 일정한 기준이나 범위로 변환하는 작업을 의미한다.  
  대표적으로 정규화, 표준화, 로그 변환 등이 있으며, 이를 통해 데이터의 분포를 조정하고, 머신러닝 모델이 데이터의 크기에 영향을 받지 않도록 한다.

- **로그 변환의 효과:**  
  데이터의 분포가 한쪽으로 치우쳐 있거나(비대칭), 값의 차이가 너무 클 때, 로그 변환을 적용하면 분포가 더 균등해지고, 모델의 학습이 더 안정적으로 이루어진다.  
  예를 들어, 1, 10, 100, 1000과 같은 값에 로그를 취하면 0, 1, 2, 3으로 변환되어 값의 차이가 줄어든다.  
  즉, 편차가 큰 데이터를 로그 변환하면 데이터의 분포가 개선되고, 값의 범위 차이가 줄어들어 머신러닝 모델이 보다 안정적으로 학습할 수 있다.  
  이는 이상치의 영향을 줄이고, 모델의 예측 성능을 높이는 데에도 도움이 된다.

- **상용 로그(common logarithm):**  
  밑이 10인 로그를 의미하며, log₁₀(x)로 표기한다. 데이터 분석에서 자주 사용되며, 특히 지수적 성장이나 감쇠를 나타내는 데 유용하다.

- **이진 로그(binary logarithm):**  
  밑이 2인 로그를 의미하며, log₂(x)로 표기한다. 컴퓨터 과학 분야에서 많이 사용되며, 데이터의 크기, 비트(bit) 계산, 알고리즘의 시간 복잡도 분석(예: 이진 탐색, 트리 구조 등)에 자주 활용된다.

### 5-2. 로그 변환의 장점과 데이터 전처리 방법

- **로그 변환의 장점:**  
  편차가 큰 데이터를 로그 변환하면 데이터의 분포가 개선되고, 값의 범위 차이가 줄어든다.  
  이로 인해 머신러닝 모델이 보다 안정적으로 학습할 수 있으며, 이상치의 영향을 줄이고 예측 성능을 높일 수 있다.  
  예를 들어, $[1, 10, 100, 1000]$과 같은 데이터에 로그 변환을 적용하면 $[0, 1, 2, 3]$으로 변환되어 값의 차이가 크게 줄어든다.  
  **참고:** 로그 변환은 $x \leq 0$인 값에는 바로 적용할 수 없으므로, $x+1$ 또는 $x-\min(x)+1$과 같이 변형하여 사용하기도 한다.

- **정규화(Normalization):**  
  데이터의 최소값을 0, 최대값을 1로 맞추는 작업.  
  각 값을 $$(값 - 최소) / (최대 - 최소)$$ 공식으로 계산하여 모든 데이터가 0~1 범위로 변환된다.  
  **특징:** 정규화는 이상치(outlier)에 민감할 수 있다.

- **표준화(Standardization):**  
  데이터의 평균을 0, 표준편차를 1로 맞추는 작업.  
  각 값을 $$(값 - 평균) / (표준편차)$$ 공식으로 계산하여 평균이 0, 분산이 1인 데이터로 변환된다.  
  표준편차가 1이라는 것은 데이터가 평균을 중심으로 얼마나 퍼져 있는지를 나타내며,  
  표준화된 데이터에서는 대부분의 값이 평균 0을 기준으로 -1에서 1 사이에 분포하게 된다.  
  즉, 표준편차가 1이면 데이터의 분포가 일정한 척도로 맞춰져, 서로 다른 특성(변수) 간의 비교나 머신러닝 모델 학습 시 영향을 최소화할 수 있다.  
  **특징:** 표준화는 이상치의 영향을 상대적으로 덜 받는다.

- **정규화와 표준화의 목적:**  
  두 방법 모두 데이터의 스케일을 맞추어, 서로 다른 단위를 가진 데이터를 비교하거나 머신러닝 모델의 성능을 높이기 위해 사용된다.  
  정규화는 주로 데이터의 범위를 일정하게 맞추어 주며, 표준화는 데이터의 분포를 표준 정규분포로 변환하여 평균이 0, 표준편차가 1이 되도록 한다.

- **np.exp 함수란?**  
  `np.exp`는 넘파이(NumPy)에서 제공하는 함수로, **자연상수 $e$ (약 2.718)**를 밑으로 하는 지수 함수를 계산합니다.  
  즉, $np.exp(x)$는 $e^x$를 의미합니다.  
  로그 변환의 역변환(원래 값 복원)에도 사용되며, 예를 들어 $y = \log(x)$라면 $x = \exp(y)$로 원래 값을 구할 수 있습니다.

  예시:
  ```python
  import numpy as np
  print(np.exp(1))  # 약 2.718 (e의 1제곱)
  print(np.exp(2))  # 약 7.389 (e의 2제곱)
  ```

---

## 6. 경사 하강법(Gradient Descent)

- **경사 하강법이란:**  
  머신러닝과 딥러닝에서 모델의 손실 함수(loss function)를 최소화하기 위해 사용되는 최적화 알고리즘이다.  
  함수의 기울기(경사, gradient)를 따라 반복적으로 파라미터(가중치 등)를 업데이트하여 최적의 값을 찾는다.  
  **참고:** 손실 함수가 볼록(convex)일 때는 전역 최솟값을 찾을 수 있지만, 비볼록 함수에서는 지역 최솟값에 머무를 수 있다.

- **수식:**  
  파라미터 $w$를 업데이트하는 기본 공식은 다음과 같다.

  $$
  w := w - \eta \frac{\partial L}{\partial w}
  $$

  - $w$ : 모델의 파라미터(가중치)
  - $\eta$ : 학습률(learning rate), 한 번에 이동하는 크기
  - $L$ : 손실 함수(loss function)
  - $\frac{\partial L}{\partial w}$ : 손실 함수 $L$을 $w$로 미분한 값(기울기)

- **작동 원리:**  
  1. 현재 파라미터에서 손실 함수의 기울기를 계산한다.
  2. 기울기가 가리키는 방향(손실이 증가하는 방향)의 반대로 파라미터를 조금 이동시킨다.
  3. 이 과정을 반복하면 손실 함수의 값이 점점 작아지며, 최솟값(최적점)에 수렴하게 된다.

- **예시(단일 변수):**  
  $$
  w_{new} = w_{old} - \eta \frac{dL}{dw}
  $$

- **시각적 의미:**  
  산 위에서 경사가 가장 가파른 방향으로 조금씩 내려가면서 가장 낮은 지점(최소값)을 찾는 과정과 비슷하다.

- **학습률(learning rate)의 역할:**  
  - $\eta$가 너무 크면 최솟값을 지나쳐 발산할 수 있고,  
    너무 작으면 수렴 속도가 매우 느려진다.

- **종류:**  
  - 배치 경사 하강법(Batch Gradient Descent)
  - 확률적 경사 하강법(Stochastic Gradient Descent, SGD)
  - 미니배치 경사 하강법(Mini-batch Gradient Descent)

---

### 7. [파이썬 타입 힌트와 None 사용법]

- **Optional과 None:**  
  파이썬에서 함수의 인자 타입을 지정할 때, `None`을 기본값으로 사용하려면 타입 힌트에 `Optional`을 반드시 명시해야 한다.  
  예를 들어, 아래와 같이 선언하면 `x`에 `float` 또는 `None`을 모두 넣을 수 있다.

  ```python
  from typing import Optional

  def func(x: Optional[float] = None):
      pass

  func(1.0)   # 가능
  func(None)  # 가능
  ```

- **float만 지정한 경우:**  
  만약 아래처럼 타입 힌트가 `float`로만 되어 있으면, `None`을 기본값이나 인자로 넣을 경우 타입 검사기(예: Pylance)에서 오류가 발생한다.

  ```python
  def func(x: float = None):  # 타입 검사기에서 오류 발생
      pass
  ```

- **정리:**  
  - `Optional[타입]` 또는 `타입 | None`(Python 3.10 이상)으로 선언된 곳에만 `None`을 할당할 수 있다.
  - 그냥 `float`에는 `None`을 할당하면 타입 검사기에서 오류가 발생한다.

---

## 8. 데이터 랭글링(Data Wrangling)

- **데이터 랭글링이란?**  
  데이터 랭글링(Data Wrangling, 또는 데이터 머시징/Data Munging)은  
  원시(raw) 데이터를 분석이나 모델링에 적합한 형태로 변환하고 정리하는 모든 과정을 의미한다.  
  실제 데이터는 결측치, 이상치, 불필요한 컬럼, 다양한 형식 등으로 인해 바로 사용할 수 없는 경우가 많으므로,  
  데이터 랭글링을 통해 데이터를 정제(clean), 변환(transform), 통합(integrate)하는 작업이 필요하다.

- **주요 작업**  
  - 결측치(NaN, None 등) 처리: 삭제, 대체(평균, 중앙값, 최빈값 등)
  - 이상치(outlier) 탐지 및 처리
  - 데이터 타입 변환(문자→숫자, 날짜형 변환 등)
  - 중복 데이터 제거
  - 컬럼명/인덱스 정리 및 재구성
  - 데이터 병합(merge), 집계(aggregation), 피벗(pivot) 등 구조 변환
  - 범주형 데이터 인코딩(예: 원-핫 인코딩)
  - 스케일링, 정규화, 표준화 등 수치형 데이터 변환

- **활용 예시**  
  - Pandas의 `dropna()`, `fillna()`, `astype()`, `merge()`, `groupby()`, `pivot_table()` 등 다양한 함수 활용
  - 데이터 분석, 머신러닝, 시각화 등 모든 데이터 기반 작업의 첫 단계로 필수적임

- **중요성**  
  데이터 랭글링이 잘 되어 있어야 이후의 분석, 모델링, 시각화 결과의 신뢰성과 효율성이 크게 향상된다.  
  실제 데이터 분석 프로젝트에서 데이터 랭글링이 전체 작업의 60~80%를 차지할 정도로 매우 중요한 과정이다.

---

## 9. 판다스(Pandas)와 넘파이(NumPy)의 역할

- **판다스(Pandas)란?**  
  판다스(Pandas)는 파이썬에서 데이터 분석과 조작을 위해 가장 널리 사용되는 라이브러리 중 하나입니다.  
  표 형태(엑셀과 유사)의 데이터 구조인 DataFrame과 Series를 제공하며,  
  데이터 읽기/쓰기, 정제, 변환, 집계, 병합, 피벗, 인덱싱 등 다양한 데이터 처리 기능을 지원합니다.

- **넘파이(NumPy)란?**  
  넘파이(NumPy)는 파이썬에서 고성능 수치 계산과 대규모 배열/행렬 연산을 위한 라이브러리입니다.  
  다차원 배열 객체(ndarray)와 벡터화된 연산, 선형대수, 난수 생성 등 과학/공학 계산에 필수적인 기능을 제공합니다.

- **데이터 분석에서의 역할**  
  - **넘파이**는 수치 데이터의 빠른 연산, 배열/행렬 기반의 데이터 처리에 강점을 가집니다.  
    머신러닝, 통계, 신호처리 등에서 대규모 수치 데이터를 다룰 때 주로 사용됩니다.
  - **판다스**는 표 형태(행/열)의 데이터 분석, 결측치 처리, 그룹화, 집계, 시계열 데이터 처리 등  
    실제 데이터 분석 업무에서 가장 많이 활용됩니다.  
    데이터 전처리, 랭글링, EDA(탐색적 데이터 분석) 등에서 필수적입니다.

- **관계 및 활용 예시**  
  - 판다스의 DataFrame/Series는 내부적으로 넘파이의 ndarray를 기반으로 동작합니다.
  - 넘파이로 수치 연산을 빠르게 처리하고, 판다스로 데이터 구조화 및 분석을 수행하는 것이 일반적입니다.
  - 예시:  
    ```python
    import pandas as pd
    import numpy as np

    # 넘파이 배열 생성
    arr = np.array([[1, 2], [3, 4]])

    # 판다스 DataFrame으로 변환
    df = pd.DataFrame(arr, columns=['A', 'B'])

    # 데이터 집계 및 분석
    print(df.describe())
    ```

- **정리:**  
  - 넘파이(NumPy)는 **사칙연산, 행렬 연산, 벡터 연산 등 기초적이고 빠른 수치 계산**을 담당합니다.  
    즉, 데이터를 빠르게 계산하고 처리하는 "기초 연산 엔진" 역할입니다.
  - 판다스(Pandas)는 **넘파이 기반 위에서 데이터 구조화(표 형태), 정제, 집계, 변환, 분석 등**  
    실제 데이터 분석(미적분, 통계, 데이터 가공 등)에 가까운 **고차원 작업**을 담당합니다.
  - 판다스는 내부적으로 넘파이를 활용하여 동작하며,  
    실제 데이터 분석에서는 두 라이브러리를 함께 사용하는 경우가 대부분입니다.

  > **비유:**  
  > - 넘파이: 빠르고 효율적인 "기초 연산(사칙연산, 행렬연산 등)"  
  > - 판다스: 실제 데이터 분석에 필요한 "고차원 데이터 처리(미적분, 통계, 집계 등)"  
  > - 판다스는 넘파이 위에서 동작하는 데이터 분석 도구입니다.

## 10. 알고리즘
- 알고리즘을 몰라도 데이터 분석에는 문제 없음
- 개발 분야에 종사하기 위해서는 알고리즘을 알아야 함.
