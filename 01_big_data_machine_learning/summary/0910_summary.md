# 09/10 수업 내용 정리

## 1. 심층 신경망(Deep Neural Network)
여러 층(layer)으로 구성된 인공신경망을 의미하며, 입력층-은닉층-출력층 구조를 가진다. 은닉층이 많아질수록 더 복잡한 패턴을 학습할 수 있다. 심층 신경망은 이미지, 음성, 자연어 등 다양한 분야에서 뛰어난 성능을 보여준다.

## 2. 신경망의 발전계기
1950년대에 인공신경망의 기본 개념이 등장했지만, 2000년대 중반까지는 크게 주목받지 못했다. 그 이유는 다음과 같다:
- **자원 부족**: GPU 등 대규모 연산을 빠르게 처리할 수 있는 하드웨어가 부족했다.
- **데이터 부족**: 충분한 양질의 데이터가 없어서 모델의 성능을 높이기 어려웠다.
- **알고리즘 한계**: 역전파(Backpropagation)와 최적화(Optimization) 등 효율적인 학습 알고리즘이 개발되지 않았거나 널리 쓰이지 않았다.

## 3. 순방향 학습(Forward Propagation)
입력 데이터를 신경망의 각 층을 거쳐 출력값을 계산하는 과정입니다. 입력층에서 시작해 은닉층을 거쳐 출력층까지 순차적으로 전달되며, 각 층에서는 가중치와 편향, 활성화 함수가 적용되어 예측값이 만들어집니다. 이 과정은 오차(실제값과 예측값의 차이)를 계산하기 위한 단계로, 이후 역전파를 통해 가중치가 조정됩니다.

## 4. 역전파(Backpropagation) = 역방향 학습
신경망 학습의 핵심 알고리즘으로, 출력값과 실제값의 오차를 계산해 이를 각 층의 가중치에 거꾸로(역방향) 전달하여 가중치를 조정한다. 이 방법 덕분에 깊은 신경망도 효과적으로 학습할 수 있게 되었다.

## 5. 딥러닝(Deep Learning)
반도체 기술 발전으로 GPU가 상용화되고, 인터넷과 정보화 사회의 발전으로 대규모 데이터가 쉽게 수집 가능해졌다. 또한 다양한 수학적 기법과 학습 알고리즘이 개발되면서 인공신경망의 한계를 극복할 수 있었다. 그 결과, '깊이 학습한다'는 의미의 딥러닝(Deep Learning)이라는 용어가 등장했고, 인공신경망은 이미지·음성·언어 등 다양한 분야에서 혁신적인 성과를 내고 있다.

### (참고1) 원-핫 인코딩(One-Hot Encoding)
원-핫 인코딩은 범주형 데이터를 0과 1로 이루어진 벡터로 변환하는 대표적인 방법이다. 예를 들어, 0~9까지의 숫자 중 '1'을 표현할 때 [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]처럼 해당 위치만 1이고 나머지는 모두 0으로 표시한다.

이 방식은 신경망에서 분류 문제를 다룰 때 매우 중요하다. 특히 교차 엔트로피 오차(Cross Entropy Error)와 같은 손실 함수를 사용할 때, 정답(label)을 원-핫 인코딩 형태로 변환해야 정확한 학습이 가능하다.

예시)
- 숫자 '3'을 원-핫 인코딩: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
- '고양이', '강아지', '토끼' 3종 분류라면 '강아지'는 [0, 1, 0]로 표현

이렇게 하면 신경망이 각 클래스(범주)를 명확하게 구분할 수 있다.

## (참고2) 교차 엔트로피 오차(Cross Entropy Error)
교차 엔트로피 오차는 분류 문제에서 모델의 예측값과 실제값 간의 차이를 측정하는 손실 함수이다. 주로 소프트맥스(Softmax) 함수와 함께 사용되며, 모델이 예측한 확률 분포와 실제 레이블 간의 불일치를 최소화하는 방향으로 학습이 진행된다.

## 6. Keras

Keras는 파이썬 기반의 딥러닝 라이브러리로, 신경망 모델을 쉽고 빠르게 만들 수 있도록 직관적인 API를 제공한다.  
TensorFlow, Theano, CNTK 등 다양한 백엔드 엔진 위에서 동작하며, 복잡한 신경망 구조도 간단한 코드로 구현할 수 있다.  
모듈화된 구조(층, 모델, 활성화 함수, 손실 함수 등) 덕분에 실험과 프로토타이핑에 적합하며, 초보자부터 전문가까지 널리 사용된다.  
현재는 TensorFlow에 통합되어 공식 문서와 튜토리얼이 TensorFlow 사이트에 마련되어 있다. Keras의 주요 특징은 다음과 같다:
- **간결성**: 직관적이고 간결한 API로 빠른 모델 구축 가능
- **유연성**: 다양한 백엔드 엔진 지원으로 유연한 모델 설계 및 배포
- **모듈화**: 구성 요소가 모듈화되어 있어 필요에 따라 쉽게 변경 및 확장 가능
- **대규모 지원**: 대규모 데이터와 모델을 위한 분산 학습 및 GPU 가속 지원

Keras를 사용하면 복잡한 딥러닝 모델도 비교적 간단한 코드로 구현할 수 있어, 연구자와 개발자 모두에게 유용한 도구이다.

## 7. Dense, Conv, RNN, Conv2d, LSTM의 관계

  - **Dense(완전연결층)**: 모든 입력 뉴런이 모든 출력 뉴런과 연결되는 기본적인 신경망 구조. 주로 일반적인 분류, 회귀 문제에 사용.
  - **Conv(합성곱층, Conv2d)**: 이미지나 시계열 데이터에서 특징을 추출하는 데 사용. Conv2d는 2차원 이미지에 적용되는 합성곱 연산을 의미.
  - **RNN(순환신경망)**: 시계열, 자연어 등 순서가 중요한 데이터에 사용. 이전 단계의 출력을 다음 단계의 입력으로 활용해 시간적 패턴을 학습.
  - **LSTM(Long Short-Term Memory)**: RNN의 한 종류로, 장기 의존성 문제를 해결하기 위해 고안된 구조. 긴 시퀀스 데이터에서도 효과적으로 학습 가능.

  - 관계:  
    Dense는 모든 신경망의 기본 구조이고, Conv/Conv2d는 이미지 등 공간적 특징 추출에 특화, RNN/LSTM은 시간적 패턴이나 순차적 데이터에 특화되어 있다. 실제 모델에서는 이들 층을 조합해 복잡한 문제를 해결한다.

## 8. Keras 기본 개념 및 모델링 순서

### Keras의 핵심 구조
- Keras의 가장 핵심적인 데이터 구조는 **모델(model)**이다.
- 대표적으로 `Sequential` 모델을 사용해 레이어를 순차적으로 쉽게 쌓을 수 있다.
- `Sequential`에 `Dense`(완전연결층) 등 다양한 레이어를 스택 구조로 추가해 신경망을 구성한다.

### Keras 모델링 순서

1. **데이터 세트 생성**
   - 원본 데이터를 불러오거나 생성한다.
   - 훈련세트, 검증세트, 시험세트로 분할하고, 딥러닝 모델 학습에 맞게 포맷을 변환한다.

2. **모델 구성**
   - `Sequential` 모델을 생성한 뒤 필요한 레이어를 추가한다.
   - 복잡한 모델이 필요할 때는 Keras 함수형 API를 이용한다.

   ```python
   from keras.models import Sequential
   from keras.layers import Dense

   model = Sequential()
   model.add(Dense(1, input_dim=3, activation='relu'))  # 3개의 입력, 1개의 출력 뉴런, ReLU 활성화 함수
   ```

   - `Dense()` 주요 인자:
     - 첫 번째 인자: 출력 뉴런의 수
     - `input_dim`: 입력 뉴런의 수(입력 차원)
     - `activation`: 활성화 함수 (`linear`, `sigmoid`, `softmax`, `relu` 등)

3. **모델 학습 과정 설정**
   - 학습 전 손실 함수와 최적화 방법을 정의한다.
   - `compile()` 함수로 설정

   ```python
   model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
   # 또는
   model.compile(optimizer=tf.keras.optimizers.RMSprop(0.01),
                 loss=tf.keras.losses.CategoricalCrossentropy(),
                 metrics=[tf.keras.metrics.CategoricalAccuracy()])
   ```

   - 주요 속성:
     - `optimizer`: 훈련 과정의 최적화 방법 (`adam`, `sgd`, `RMSprop` 등)
     - `loss`: 손실 함수
     - `metrics`: 훈련 모니터링 지표

4. **모델 학습시키기**
   - 훈련 데이터를 이용해 모델을 학습시킨다. `fit()` 함수 사용

   ```python
   model.fit(X_train, y_train, epochs=10, batch_size=32)
   ```

   - 주요 인자:
     - 첫 번째 인자: 훈련 데이터
     - 두 번째 인자: 레이블 데이터
     - `epochs`: 전체 데이터를 몇 번 반복 학습할지(에포크 수)
     - `batch_size`: 미니 배치 크기(기본값 32)

5. **학습과정 살펴보기**
   - 훈련/검증 세트의 손실 및 정확도 추이 확인

6. **모델 평가**
   - 시험 세트로 평가 (`evaluate()` 함수)

   ```python
   model.evaluate(X_test, y_test, batch_size=32)
   ```

7. **모델 사용하기**
   - 예측 (`predict()` 함수)

   ```python
   model.predict(X_input, batch_size=32)
   ```

---

## 9. 딥러닝 데이터셋과 모델 학습의 실제

딥러닝 모델을 학습시키려면 적절한 **데이터셋(dataset)**이 필요하다.  
학습하려는 문제와 만들고자 하는 모델에 따라 데이터셋의 설계와 분할 방식이 달라진다.  
일반적으로 데이터셋은 아래와 같이 나뉜다.

- **Train dataset**: 모델을 학습시키는 데 사용되는 데이터(예: 모의고사 5회분)
- **Test dataset**: 학습된 모델의 성능을 평가하는 데 사용되는 데이터(예: 작년 수능 문제)
- **Validation dataset**: 학습 중 모델의 성능을 검증하는 데 사용되는 데이터(선택적)
- **Real dataset**: 실제로 경험해보지 않은 새로운 데이터(예: 올해 수능 문제)

학습의 의미는 문제와 정답을 같이 주고, 모델이 문제를 풀고 정답과 비교하며 성능을 향상시키는 과정이다.  
평가의 의미는 정답 없이 문제만 주고, 모델이 예측한 결과를 실제 정답과 비교해 점수만 계산하는 과정이다.

### 배치 사이즈(batch_size)란?

- **batch_size**는 한 번에 몇 개의 데이터를 처리하고 가중치를 갱신할지 결정하는 값이다.
- 예를 들어, 100문제를 한 번에 풀고 정답을 맞추면 가중치 갱신은 한 번만 일어난다.
- batch_size가 1이면 한 문제씩 풀고 바로 정답을 확인하며 가중치를 갱신한다.
- batch_size가 크면 학습이 빠르지만 메모리 사용량이 많아지고, batch_size가 작으면 학습이 꼼꼼하지만 시간이 오래 걸린다.
- 적절한 batch_size 선택은 모델 성능과 학습 효율에 중요한 영향을 준다.

### 데이터셋 반복과 전처리

- 동일한 유형의 데이터를 여러 번 반복해서 학습하는 것과 다양한 유형의 데이터를 한 번씩 학습하는 것은 서로 다른 학습 효과를 낸다.
- 양질의 데이터를 잘 전처리하고, 반복적으로 모델을 학습시키는 것이 실제로 더 효율적일 수 있다.

### 실제 예시

- 학생이 여러 번 모의고사를 풀면서 실력을 키우는 과정과 유사하게, 딥러닝 모델도 반복 학습과 평가를 통해 성능을 높인다.
- 모델은 결과값과 라벨값의 오차를 줄이기 위해 **역전파(Backpropagation)** 알고리즘을 사용한다.

> 참고: 실제 프로젝트에서는 데이터셋의 분할, 전처리, batch_size 설정 등이 모델 성능에 큰 영향을 미치므로, 데이터 준비와 실험 설계가 매우 중요하다.

## 10. Epoch, Batch size, Iteration의 개념

딥러닝에서 모델의 학습 과정은 실제값과 예측값의 오차를 기반으로 옵티마이저가 가중치를 업데이트하는 반복적인 과정입니다.  
사람이 문제를 풀고 정답을 채점하며 지식을 업데이트하는 것과 유사하게, 컴퓨터도 데이터를 여러 방식으로 학습할 수 있습니다.

- **Epoch(에포크)**  
  전체 데이터셋을 한 번 모두 학습(순전파와 역전파)한 상태를 의미합니다.  
  예를 들어, 에포크가 50이면 전체 데이터를 50번 반복해서 학습합니다.  
  에포크가 너무 많거나 적으면 과적합(overfitting) 또는 과소적합(underfitting)이 발생할 수 있습니다.

- **Batch size(배치 크기)**  
  한 번에 몇 개의 데이터 단위로 가중치를 업데이트할지 결정하는 값입니다.  
  예를 들어, 전체 데이터가 2,000개이고 배치 크기가 200이면, 200개씩 문제를 풀고 채점하는 것과 같습니다.  
  배치 크기가 크면 학습이 빠르지만 메모리 사용량이 많아지고, 작으면 학습이 꼼꼼하지만 시간이 오래 걸립니다.

- **Iteration(이터레이션)**  
  한 번의 에포크를 끝내기 위해 필요한 배치의 수, 즉 매개변수 업데이트 횟수입니다.  
  전체 데이터가 2,000개, 배치 크기가 200이면 이터레이션은 10번입니다.  
  SGD(확률적 경사 하강법)는 배치 크기가 1이므로 모든 이터레이션마다 하나의 데이터로 업데이트합니다.

---

## 11. 과적합(Overfitting)과 Regularization(정규화)

딥러닝 모델은 훈련 데이터에 너무 특화되면 새로운 데이터에 대한 일반화 성능이 떨어질 수 있습니다. 이를 **과적합**이라고 하며, 이를 막기 위한 여러 방법이 있습니다.

- **조기 종료(Early Stopping)**: 검증 데이터의 성능이 더 이상 개선되지 않으면 학습을 중단
- **드롭아웃(Dropout)**: 학습 중 일부 뉴런을 임의로 제거하여 모델의 복잡도를 줄임
- **가중치 규제(Weight Regularization)**: 네트워크의 복잡도에 제한을 두어 가중치가 작은 값을 가지도록 강제
  - **L1 규제**: 가중치의 절댓값에 비례하는 비용 추가 (L1 norm)
  - **L2 규제(Weight Decay)**: 가중치의 제곱에 비례하는 비용 추가 (L2 norm)

```python
from tensorflow.keras import regularizers, layers
model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001), activation='relu', input_shape=(10000,)))
```
- 위 예시에서 `regularizers.l2(0.001)`은 가중치 행렬의 모든 원소를 제곱하고 0.001을 곱해 전체 손실에 더합니다.

---

## 12. Regularization과 딥러닝의 일반적인 흐름

- **최적화(Optimization)**: 훈련 데이터에서 최고의 성능을 얻기 위해 모델을 조정하는 과정
- **일반화(Generalization)**: 훈련된 모델이 새로운 데이터에서 얼마나 잘 수행되는지 평가

  - **과소적합(Underfitting)**: 훈련 데이터와 테스트 데이터 모두에서 손실이 높음. 모델이 충분히 학습하지 못한 상태.
  - **과대적합(Overfitting)**: 훈련 데이터에서는 손실이 낮지만, 테스트 데이터에서는 손실이 높아짐. 모델이 훈련 데이터에만 특화된 상태.

**Regularization(정규화)**는 과대적합을 막기 위한 대표적인 처리 과정입니다.
- 더 많은 훈련 데이터를 모으는 것이 가장 좋은 방법
- 모델의 파라미터 수(층 수, 유닛 수)를 줄이거나, 가중치에 제약을 가하는 방법
- Dropout, L1/L2 규제 등 다양한 기법을 조합해 모델의 일반화 성능을 높임

> 참고: [파라미터 튜닝 가이드](https://cloud.google.com/ai-platform/training/docs/using-hyperparameter-tuning), [하이퍼파라미터 검색](https://mlfromscratch.com/gridsearch-keras-sklearn/#/)

---

## 13. 정규화의 철학

- 오캄의 면도날(Occam's Razor): 복잡한 모델보다 간단한 모델이 더 일반화 성능이 높을 수 있음
- 신경망에서는 가중치가 작은 값을 가지도록 네트워크의 복잡도에 제약을 가하는 것이 일반적
- 손실 함수에 큰 가중치에 해당하는 비용을 추가하여, 모델이 불필요하게 복잡해지는 것을 방지

---

> 요약: 에포크, 배치 크기, 이터레이션은 딥러닝 학습의 기본 단위이며, Regularization은 과적합을 막아 모델의 일반화 성능을 높이는 핵심 기법입니다.

## 14. 손실 함수(Loss function)

손실 함수는 실제값과 예측값의 차이를 수치화하는 함수입니다.  
오차가 클수록 손실 함수 값이 크고, 오차가 작을수록 값이 작아집니다.

- **MSE(Mean Squared Error)**: 회귀 문제에서 사용. 오차 제곱 평균.
- **Cross-Entropy**: 분류 문제에서 사용. 예측 확률과 실제값의 불일치 정도를 측정.
  - 이진 분류: `binary_crossentropy`
    - 두 개의 클래스(예: 스팸/비스팸, 암/정상 등)를 구분하는 문제에 적합
    - 출력층에 sigmoid 활성화 함수를 사용하며, 정답(label)은 0 또는 1로 표현
    - 예시: 이메일이 스팸인지 아닌지, 환자가 질병이 있는지 없는지

  - 다중 분류: `categorical_crossentropy`
    - 세 개 이상의 클래스(예: 고양이/강아지/토끼 등)를 구분하는 문제에 적합
    - 출력층에 softmax 활성화 함수를 사용하며, 정답(label)은 원-핫 인코딩 벡터로 표현
    - 예시: 이미지가 10종류 중 어느 클래스에 속하는지, 뉴스 기사 주제 분류 등

손실 함수의 값을 최소화하는 가중치(W)와 편향(b)를 찾는 것이 딥러닝의 학습 과정입니다.

---

## 15. Keras 샘플 데이터

Keras는 다양한 샘플 데이터를 제공합니다.

- CIFAR10, CIFAR100: 이미지 분류용 데이터셋
- IMDB: 영화 감상 텍스트 데이터
- Reuters: 뉴스 토픽 분류 데이터
- MNIST: 숫자 이미지 데이터
- Fashion MNIST: 의류 이미지 데이터
- Boston housing price: 회귀용 주택 가격 데이터

---

## 16. 모델 시각화 및 예제 코드

- `plot_model()`을 사용하면 Sequential 모델을 시각화할 수 있습니다.

```python
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='model.png', show_shapes=True)
```

- [실제 예제: iris 데이터 분류](https://keras.io/examples/iris_classification/)
- [PyTorch 예제 코드 비교](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html)


## 17. 활성화 함수의 종류

신경망에서 활성화 함수는 각 뉴런의 출력값을 결정하는 함수로, 비선형성을 부여해 복잡한 패턴을 학습할 수 있게 해준다. 주요 활성화 함수와 특징은 다음과 같다:

- **Sigmoid**
  - 출력값을 0~1 사이로 변환
  - 이진 분류 문제의 출력층에 주로 사용
  - 단점: 입력값이 크거나 작을 때 기울기가 거의 0이 되어 학습이 느려질 수 있음(기울기 소실)

- **Tanh(하이퍼볼릭 탄젠트)**
  - 출력값을 -1~1 사이로 변환
  - 은닉층에서 많이 사용
  - Sigmoid보다 중심이 0에 가까워 학습이 더 잘 되는 경우가 많음

- **ReLU(Rectified Linear Unit)**
  - 0보다 작으면 0, 크면 그대로 출력
  - 은닉층에서 가장 널리 사용되는 함수
  - 계산이 간단하고, 기울기 소실 문제를 완화
  - 단점: 입력이 0 이하일 때 뉴런이 죽는 현상(Dead Neuron)

- **Leaky ReLU, Parametric ReLU**
  - ReLU의 단점을 보완한 함수로, 0 이하일 때도 작은 기울기를 갖게 함

- **Softmax**
  - 다중 클래스 분류 문제의 출력층에 사용
  - 각 클래스별 확률값(합이 1)을 출력

- **Linear**
  - 별도의 활성화 함수 없이 입력값을 그대로 출력
  - 회귀 문제의 출력층에 사용

