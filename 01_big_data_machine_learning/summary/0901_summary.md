# 0901 수업 내용

## 1. Decision Tree(결정 트리)에 대해서 학습함

- **결정 트리(Decision Tree)**는 데이터를 여러 기준에 따라 분할하여 예측을 수행하는 지도 학습 알고리즘입니다.
- 트리 구조로 이루어져 있으며, 각 **노드(node)**는 하나의 특성(feature)에 대한 조건(질문)을 나타냅니다.
- **루트 노드(root node)**에서 시작해 조건에 따라 데이터를 분할하며, **잎 노드(leaf node)**에 도달하면 최종 예측값(클래스 또는 회귀값)을 출력합니다.

### 주요 특징
- **직관적이고 해석이 쉬움**: 트리 구조로 분류/회귀 과정을 시각적으로 이해할 수 있습니다.
- **범주형/수치형 데이터 모두 처리 가능**
- **특성 선택 자동화**: 정보 이득(Information Gain), 지니 지수(Gini Index) 등 기준으로 분할할 특성을 자동으로 선택함.
- **과적합(overfitting) 위험**: 트리가 너무 깊어지면 학습 데이터에만 맞춰져 일반화 성능이 떨어질 수 있음. 이를 방지하기 위해 트리의 깊이(max_depth), 최소 샘플 수(min_samples_split) 등 하이퍼파라미터를 조정함.

### 분할 기준
- **분류 문제**: 지니 지수(Gini Index), 엔트로피(Entropy) 등으로 분할
- **회귀 문제**: 평균 제곱 오차(MSE) 등으로 분할

### 장점과 단점
- **장점**: 해석 용이, 전처리 적음, 다양한 데이터에 적용 가능
- **단점**: 과적합 위험, 작은 변화에도 트리 구조가 크게 바뀔 수 있음

### 활용 예시
- 고객 이탈 예측, 질병 진단, 신용 평가 등 다양한 분야에서 사용

### 대표 구현
- Scikit-learn의 `DecisionTreeClassifier`

## 2. 과적합을 막는 방법

- **KFold 교차검증(KFold Cross Validation)**
  - 데이터를 여러 개의 폴드(fold)로 나누어, 각 폴드가 한 번씩 테스트셋이 되고 나머지는 학습셋이 되어 반복적으로 모델을 평가하는 방법입니다.
  - 모든 데이터가 학습과 평가에 고르게 사용되어, 모델의 일반화 성능을 높이고 과적합을 방지할 수 있습니다.
  - 대표적으로 `KFold`, `StratifiedKFold` 등이 있습니다.

- **GridSearchCV**
  - 여러 하이퍼파라미터 조합을 체계적으로 탐색하여, 가장 성능이 좋은 파라미터를 자동으로 선택해주는 방법입니다.
  - 각 파라미터 조합마다 교차검증을 수행하여, 과적합을 방지하고 최적의 모델을 찾을 수 있습니다.
  - Scikit-learn의 `GridSearchCV`를 사용하면 손쉽게 적용할 수 있습니다.

## 3. 앙상블 학습 방식

- **VOTING**
  - 여러 개의 서로 다른 모델(분류기)을 학습시켜, 각 모델의 예측 결과를 투표로 결정하는 방식입니다.
  - 분류 문제에서는 다수결로 최종 클래스를 선택하고, 회귀 문제에서는 평균값을 사용합니다.
  - 서로 다른 알고리즘(예: 결정트리, 로지스틱 회귀, SVM 등)을 결합해 예측 성능을 높일 수 있습니다.

- **BAGGING (Bootstrap Aggregating)**
  - 하나의 알고리즘을 여러 번, 각기 다른 데이터 샘플(중복 허용, 부트스트랩 샘플)로 학습시켜 결과를 합치는 방식입니다.
  - 대표적으로 Random Forest가 Bagging의 예입니다.
  - 데이터의 다양성을 활용해 과적합을 줄이고, 모델의 안정성과 정확도를 높입니다.

- **BOOSTING**
  - 여러 약한 학습기(weak learner)를 순차적으로 학습시키고, 이전 모델이 틀린 부분에 가중치를 두어 다음 모델이 더 잘 학습하도록 하는 방식입니다.
  - 대표적으로 AdaBoost, Gradient Boosting, XGBoost 등이 있습니다.
  - 오차를 반복적으로 보완해가며 강한 학습기를 만드는 것이 특징입니다.

---

### 앙상블 방식 비교 분석

| 방식      | 주요 특징                                   | 장점                           | 단점                          | 대표 알고리즘         |
|-----------|--------------------------------------------|--------------------------------|-------------------------------|-----------------------|
| Voting    | 여러 다른 모델의 예측을 결합(다수결/평균)   | 다양한 모델 결합, 해석 쉬움     | 모델 간 상호작용 미고려        | VotingClassifier      |
| Bagging   | 같은 모델을 여러 데이터 샘플로 반복 학습    | 과적합 감소, 안정성↑           | 계산량 많음, 해석 어려움       | Random Forest         |
| Boosting  | 이전 오차를 보완하며 순차적 학습           | 높은 성능, 오차 보완            | 과적합 위험, 파라미터 민감     | AdaBoost, XGBoost    |

- **Voting**은 서로 다른 알고리즘을 결합해 예측 성능을 높이며, 해석이 쉽고 구현이 간단합니다.
- **Bagging**은 데이터 샘플을 다양하게 활용해 모델의 분산을 줄이고, 과적합을 방지합니다. 대표적으로 Random Forest가 있습니다.
- **Boosting**은 약한 모델을 순차적으로 학습시켜 오차를 반복적으로 보완하며, 높은 예측 성능을 얻을 수 있지만 과적합과 파라미터 설정에 민감합니다.

## 4. 하드 보팅, 소프트 보팅

- **하드 보팅(Hard Voting)**
  - 각 분류기가 예측한 **최종 클래스(레이블)**에 대해 다수결로 결과를 결정합니다.
  - 예: 3개 모델 중 2개가 'A'를 예측하면 최종 결과도 'A'로 선택.
  - 각 모델의 예측값만 사용하며, 예측 확률은 고려하지 않습니다.

- **소프트 보팅(Soft Voting)**
  - 각 분류기가 예측한 **클래스별 확률(예측 확률)**을 평균내어, 가장 높은 확률을 가진 클래스를 최종 결과로 선택합니다.
  - 예: 3개 모델의 'A' 클래스 확률 평균이 가장 높으면 'A'로 선택.
  - 모델의 신뢰도(확률 정보)를 반영하므로, 일반적으로 하드 보팅보다 성능이 더 좋을 수 있습니다.

**요약:**  
- 하드 보팅: 다수결(예측값 기준)
- 소프트 보팅: 확률 평균(예측 확률 기준)