### 1. (2교시) 통계 기초.pdf 68페이지 비선형회귀분석(Non-linear regression)

- **비선형회귀분석(Non-linear regression) 개념 설명**
    - 기존의 **선형 회귀선(직선)** 대신 **곡선 형태의 회귀선**을 사용하여 데이터의 추세를 더 정확하게 반영함
    - 직선으로 회귀선을 그릴 경우 **오차가 커질 수 있어**, 곡선 회귀선을 적용하면 **오차가 줄어듦**
    - 수업에서는 **그래프를 통해 직선(적색)과 곡선(청색) 회귀선의 차이**를 비교하며 설명함
    - **곡선이 데이터의 분포와 추세를 더 잘 따라가는 모습**을 확인함

---

- **실습 요약**
    - `lm14nonlinear.py`:  
      - **단순 입력 데이터(5개)**를 사용하여 곡선 회귀선의 적용 예시를 실습  
      - **결론:** 곡선 회귀선이 직선 회귀선보다 데이터의 추세를 더 잘 반영함
    - `lm15nonlinear.py`:  
      - **복잡한 입력 데이터(10개)**를 사용하여 직선과 곡선 회귀선을 비교 분석  
      - **결론:** 곡선 회귀선이 직선 회귀선보다 데이터의 추세를 더 잘 반영함

---

### 2. (3교시) 보스턴 집값 데이터 셋
- **보스턴 집값 데이터셋이란**
    - 보스턴 지역의 주택 가격 정보를 담고 있는 데이터셋
    - 다양한 특성(예: 방 개수, 면적, 범죄율 등)과 해당 주택의 가격 정보가 포함됨
- **다항회귀 분석 실습에 활용**
    - 보스턴 집값 데이터도 곡선의 형태를 띈다.
    - 추가로 mpg 연비 데이터셋에서도 HP(마력)은 곡선의 형태를 띈다.

- **실습 요약**
    - `lm16.py`:  
      - lm15nonlinear.py 보다 더욱 복잡한 보스턴 집값 데이터셋을 사용
      - 선형회귀, 다항회귀(degree = 2), (degree = 3)을 비교하여 시각화 실습

- **문제 풀이**
    - https://cafe.daum.net/flowlife/SBU0/44
    - 다항회귀분석 문제) 풀이 진행
    - `문제1.py`:
        - 3D 산점도로 시각화해야 함

### 3. 로지스틱 회귀
Logistic Regression (https://cafe.daum.net/flowlife/SBU0/16)

Logistic Regression - 확률과 가능도 (https://cafe.daum.net/flowlife/SBU0/67)

확률, 가능도와 함수에 관한 기본 이해 (https://cafe.daum.net/flowlife/SBU0/70)- > 확률이란.pdf

- **로지스틱 회귀(Logistic Regression)란?**
    - 분류 문제에서 널리 사용되는 지도학습 알고리즘
    - 종속 변수(y)가 범주형일 때 사용 (이항 또는 다항 분류)
    - 예시: 이메일 스팸/비스팸, 환자 질병 유무 등

- **이항 분류(Binary Classification)**
    - 결과가 두 가지(예: 0 또는 1)
    - 모델은 입력값에 대해 0~1 사이의 확률을 예측
    - 임계값(보통 0.5) 이상이면 한 클래스로, 아니면 다른 클래스로 분류

- **확률 계산 방식**
    - 이항 분류에서는 **sigmoid 함수**를 사용하여 예측값을 확률로 변환
    - sigmoid 함수:  
      \[
      \sigma(z) = \frac{1}{1 + e^{-z}}
      \]
    - z는 입력 변수의 선형 조합(wx+b, 즉 로짓)

- **다항 분류(Multiclass Classification)**
    - 클래스가 3개 이상일 때는 **softmax 함수**를 사용
    - 각 클래스별 확률을 계산하여, 가장 높은 확률의 클래스로 분류

- **활용 예시**
    - 의료 진단, 금융 사기 탐지, 마케팅(구매/비구매 예측), 텍스트 분류 등
    - 머신러닝에서 기본이 되는 분류 알고리즘

> 로지스틱 회귀에서 wx+b로 얻은 값(로짓)은 −∞부터 +∞ 사이의 값이다.  
> 이를 sigmoid 함수에 넣으면 값이 0~1 사이의 확률값으로 변환된다.  
> 이 확률값을 기준(일반적으로 0.5)으로 0 또는 1 클래스로 분류할 수 있다.

### 4. 퍼셉트론(Perceptron)
- 1개만 있으면 퍼셉트론
- 여러 개 모이면 다층 퍼셉트론(MLP)
- 입력층, 은닉층, 출력층으로 구성
- 각 층의 뉴런들은 완전히 연결되어 있음

### 5. 모델의 전체 성능(Likelihood)
- 주어진 데이터에 대해 모델이 얼마나 잘 설명하는지를 나타내는 척도
- 우도(Likelihood): 주어진 데이터가 모델에 의해 생성될 확률
- 우도를 최대화하는 방향으로 모델 파라미터를 조정
- 여러 시그모이드 중 어떤 것을 택할 것인가?를 결정할 때 우도를 기준으로 선택

## 5.1 확률(Probability)와 가능도(Likelihood)의 차이
- 확률(Probability): 특정 사건이 발생할 가능성을 수치로 나타낸 것
- 가능도(Likelihood): 주어진 데이터가 특정 모델에 의해 생성될 확률
- 예를 들어, 동전을 던져 앞면이 나올 확률은 0.5이지만, 특정 동전이 주어진 데이터(예: 10번 던져 7번 앞면이 나온 경우)에 대해 얼마나 잘 설명하는지를 나타내는 것이 가능도

### 5.2 확률(Likelihood) 계산 방법
- 우도는 주어진 데이터가 모델에 의해 생성될 확률을 나타내므로, 모델의 파라미터에 따라 달라진다.
- 일반적으로 최대 우도 추정(Maximum Likelihood Estimation, MLE)을 사용하여 모델 파라미터를 추정한다.
- MLE는 주어진 데이터에 대해 우도를 최대화하는 파라미터 값을 찾는 방법이다.

### 5.3 Log-Likelihood
- Likelihood에 로그를 씌우면 곱셈이 덧셈으로 바뀌어서 계산이 쉬워진다.
- 너무 작은 값을 다룰 때 발생하는 수치 불안정성도 줄일 수 있다.
- 이를 실제로는 부호를 바꾼 손실 함수(Loss)를 최소화하여 학습한다.

### 5.3 Log-Likelihood

- **Log-Likelihood란?**
    - 여러 데이터의 우도(Likelihood)를 곱하면 값이 매우 작아지고 계산이 복잡해진다.
    - 로그를 취하면 곱셈이 덧셈으로 바뀌어 계산이 간편해진다.
    - 로그를 취함으로써 매우 작은 값(확률)을 다룰 때 발생하는 수치적 불안정성도 줄일 수 있다.
    - 실제 머신러닝 모델 학습에서는 **로그 우도(Log-Likelihood)의 부호를 바꾼 손실 함수(Log Loss, Cross-Entropy Loss)**를 최소화하는 방식으로 파라미터를 최적화한다.

- **수식 예시**
    - 데이터가 N개일 때, 전체 우도:  
      \( L = \prod_{i=1}^{N} p(y_i | x_i) \)
    - 로그 우도:  
      \( \log L = \sum_{i=1}^{N} \log p(y_i | x_i) \)
    - 손실 함수(Log Loss):  
      \( -\log L = -\sum_{i=1}^{N} \log p(y_i | x_i) \)

---

### 5.4 로지스틱 회귀의 학습 절차

| 단계 | 설명 |
|------|------|
| ① | 입력값에 대해 wx + b(선형 결합)를 계산 |
| ② | wx + b를 **시그모이드 함수**에 넣어 0~1 사이의 확률로 변환 |
| ③ | 각 샘플별로 예측 확률을 기반으로 **가능도(Likelihood)** 계산 |
| ④ | 전체 데이터에 대한 **가능도 함수**를 정의 (모든 샘플의 가능도 곱) |
| ⑤ | 계산 편의를 위해 **로그 가능도(Log-Likelihood)**로 변환 |
| ⑥ | 로그 가능도의 부호를 반전시켜 **손실 함수(Log Loss, Cross-Entropy Loss)** 정의 |
| ⑦ | 이 손실 함수를 **최소화**하는 방향으로 w, b를 학습 (최적화) |

- **요약:**  
  - 입력 → 선형 결합 → 시그모이드 → 확률 → 가능도 → 로그 가능도 → 손실 함수 → 최소화  
  - 이렇게 해서 로지스틱 회귀 모델의 파라미터(w, b)를 최적화한다.

### 5.5 Odds Ratio

- **Odds(오즈, 승산)**  
    - 어떤 사건이 일어날 확률을, 일어나지 않을 확률로 나눈 값  
    - 예시: 사건이 일어날 확률이 0.8이라면, 오즈는 0.8 / 0.2 = 4  
    - 즉, 사건이 일어날 가능성이 일어나지 않을 가능성보다 4배 높다는 의미

- **Odds Ratio(오즈비, 승산비)**  
    - 두 집단의 오즈를 비교한 비율  
    - 예시:  
        - 집단 A의 오즈가 4, 집단 B의 오즈가 2라면  
        - 오즈비는 4 / 2 = 2  
        - 집단 A가 집단 B보다 사건이 일어날 가능성이 2배 높다는 의미
    - 로지스틱 회귀에서 계수(β)는 오즈비의 로그값으로 해석됨

---

### 5.6 Logit

- **Logit(로짓) 함수**  
    - 오즈(odds)의 로그값  
    - 식:  
      \[
      \text{logit}(p) = \log\left(\frac{p}{1-p}\right)
      \]
      - p: 사건이 일어날 확률
    - 로지스틱 회귀의 선형 결합(wx+b)은 바로 logit(p)에 해당
    - logit 함수는 확률(0~1)을 실수 전체(−∞~+∞)로 변환  
    - 반대로, logit의 역함수는 시그모이드 함수

- **요약**  
    - logit은 오즈의 로그값  
    - 로지스틱 회귀에서 예측값(wx+b)은 logit(p)로 해석  
    - logit을 시그모이드 함수로 변환하면 확률값이 됨

### 5.7 Odds Ratio → Logit → Sigmoid 함수 관계 설명

1. **Odds Ratio(오즈비, 승산비)**
    - 어떤 사건이 일어날 확률을, 일어나지 않을 확률로 나눈 값이 **오즈(odds)**임.
    - 예시: 사건이 일어날 확률이 p라면, 오즈는 \( \frac{p}{1-p} \)
    - 두 집단의 오즈를 비교한 비율이 **오즈비(odds ratio)**임.

2. **Logit(로짓) 함수**
    - 오즈의 로그값을 **로짓(logit)**이라고 함.
    - 식:  
      \[
      \text{logit}(p) = \log\left(\frac{p}{1-p}\right)
      \]
    - 로지스틱 회귀에서 입력값의 선형 결합(wx+b)은 바로 logit(p)에 해당.
    - 즉, wx+b = logit(p)

3. **Sigmoid(시그모이드) 함수**
    - logit의 역함수가 **시그모이드 함수**임.
    - 식:  
      \[
      \sigma(z) = \frac{1}{1 + e^{-z}}
      \]
    - 여기서 z = wx+b (즉, logit 값)
    - 시그모이드 함수는 logit 값을 0~1 사이의 확률로 변환함.

---

**요약 흐름**

- 입력값 → wx+b (선형 결합) → logit(p) (오즈의 로그값) → 시그모이드 함수 → 확률값(0~1)
- 즉,  
  1. 확률 → 오즈 → logit  
  2. logit → 시그모이드 → 확률

**로지스틱 회귀에서의 의미**

- 모델은 wx+b를 계산하여 logit(p)를 얻고,
- 시그모이드 함수를 통해 logit(p)를 확률값으로 변환하여 분류를 수행한다.