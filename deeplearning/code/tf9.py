# í…ì„œí”Œë¡œ gradient tape
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

# Keras.optimizers íŒ¨í‚¤ì§€ì— ìˆëŠ” Adam,SGD,RMSprop,... ì‚¬ìš©
opti=tf.keras.optimizers.SGD(learning_rate=0.01)

x=tf.Variable(5.0)
w=tf.Variable(0.0)

@tf.function
def train_step():
  # GradientTape ì—°ì‚°ê³¼ì •ì„ ê¸°ì–µí•´ë’€ë‹¤ê°€ ë‚˜ì¤‘ì— ìë™ìœ¼ë¡œ ë¯¸ë¶„(gradient)ì„ ê³„ì‚°í•¨
  with tf.GradientTape() as tape:
    y=tf.multiply(w,x) # b=0ìœ¼ë¡œ ê°„ì£¼
    loss=tf.square(tf.subtract(y,50)) # ì˜ˆì¸¡ê°’, ì‹¤ì œê°’ ë¹¼ì„œ ì œê³±
  grad=tape.gradient(loss,w) # ìë™ìœ¼ë¡œ ë¯¸ë¶„ì´ ê³„ì‚°ë¨
  opti.apply_gradients([(grad,w)])
  return loss

for i in range(10):
  loss=train_step()
  print(f' i: {i} \n w:{w.numpy()} \n loss: {loss.numpy()}')

# ì„ í˜•íšŒê·€ ëª¨í˜• ì‘ì„±
# Keras.optimizers íŒ¨í‚¤ì§€ì— ìˆëŠ” Adam,SGD,RMSprop,... ì‚¬ìš©
opti=tf.keras.optimizers.SGD(learning_rate=0.01)

x=tf.Variable(5.0)
w=tf.Variable(0.0)

@tf.function
def train_step2():
  # GradientTape ì—°ì‚°ê³¼ì •ì„ ê¸°ì–µí•´ë’€ë‹¤ê°€ ë‚˜ì¤‘ì— ìë™ìœ¼ë¡œ ë¯¸ë¶„(gradient)ì„ ê³„ì‚°í•¨
  with tf.GradientTape() as tape:
    y=tf.multiply(w,x) # b=0ìœ¼ë¡œ ê°„ì£¼
    loss=tf.square(tf.subtract(y,50)) # ì˜ˆì¸¡ê°’, ì‹¤ì œê°’ ë¹¼ì„œ ì œê³±
  grad=tape.gradient(loss,w) # ìë™ìœ¼ë¡œ ë¯¸ë¶„ì´ ê³„ì‚°ë¨
  opti.apply_gradients([(grad,w)])
  return loss

for i in range(10):
  loss=train_step2()
  print(f' i: {i} \n w:{w.numpy()} \n loss: {loss.numpy()}')

  # Keras.optimizers íŒ¨í‚¤ì§€ì— ìˆëŠ” Adam,SGD,RMSprop,... ì‚¬ìš©
# ğŸ‘‰ tf.keras.optimizers ì“°ë©´ SGD, Adam, RMSprop ë“± ê³ ê¸‰ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì„ ë°”ë¡œ í™œìš© ê°€ëŠ¥í•´ìš”.
opti=tf.keras.optimizers.SGD(learning_rate=0.01)

tf.random.set_seed(2)
w=tf.Variable(tf.random.normal((1,)))
b=tf.Variable(tf.random.normal((1,)))

@tf.function
def train_step3(x, y):
  with tf.GradientTape() as tape:
    hypo = tf.add(tf.multiply(w, x), b)
    loss = tf.reduce_mean(tf.square(tf.subtract(hypo, y)))
  grad = tape.gradient(loss, [w, b])
  opti.apply_gradients(zip(grad, [w, b]))
  return loss

x=[1.,2.,3.,4.,5.] #feature
y=[1.2,2.0,3.0,3.5,5.5] #label

w_vals=[]
cost_vals=[]

# forë¬¸ ëŒë¦¬ê³  ê²°ê³¼ ì‹œê°í™”
for i in range(1,101):
  cost_val=train_step3(x,y)
  cost_vals.append(cost_val.numpy())
  w_vals.append(w.numpy())
  if i%10==0:
    print(cost_val)
print(cost_vals)
print(w_vals)

plt.plot(w_vals,cost_vals)
plt.show()

print('costê°€ ìµœì†Œì¼ë•Œ w',w.numpy())
print('costê°€ ìµœì†Œì¼ë•Œ b',b.numpy())

y_pred=tf.multiply(x,w)+b
print('y_pred',y_pred)
plt.plot(x,y,'ro',label='real')
plt.plot(x,y_pred,'b-',label='pred')
plt.xlabel('x')
plt.ylabel('y')
# plt.show()
plt.savefig('pred.png')  # ê·¸ë˜í”„ë¥¼ íŒŒì¼ë¡œ ì €ì¥
plt.close()

# ìƒˆê°’ìœ¼ë¡œ ì˜ˆì¸¡í•˜ê¸°
new_x=[3.5,9.0]
new_pred=tf.multiply(new_x,w)+b
print('ì˜ˆì¸¡ ê²°ê³¼',new_pred.numpy())

