

# 1. 08/01 수업 + 코딩 주요 개념 요약
---
## 2. 오늘의 목표

1. 벡터의 개념과 연산(크기, 덧셈, 내적 등) 이해
2. 내적이 실제로 어떻게 계산되고, 어떤 의미를 가지는지 예시와 함께 익히기
---
## 3. 스칼라와 벡터, 속력과 속도의 차이

### 3.1 스칼라(Scalar)와 벡터(Vector)란?

1. **스칼라**: 크기만 있는 물리량 (예: 속력, 질량, 온도)
2. **벡터**: 크기와 방향이 모두 있는 물리량 (예: 속도, 힘, 가속도)

### 3.2 속력과 속도의 차이

1. **속력**: 물체가 이동한 거리(경로의 총 길이)를 걸린 시간으로 나눈 값. 방향과 무관, 크기만 있음. (예: 시속 80km)
2. **속도**: 물체의 위치 변화(변위)를 걸린 시간으로 나눈 값. 크기와 방향 모두 포함. (예: 동쪽으로 시속 80km)
---
## 4. 벡터의 표기, 연산, 예시

### 4.1 벡터의 표기
1. 보통 화살표(→) 또는 굵은 글씨로 표기: $\vec{v}$, **v**
2. 벡터의 크기(절댓값): $|\vec{v}|$, $||\vec{v}||$

### 4.2 벡터 연산
1. 여러 데이터(값)들을 벡터로 보고, 덧셈, 내적 등 다양한 연산 수행

### 4.3 벡터의 예시
1. 2차원 벡터: (3, 4) → 크기: $\sqrt{3^2 + 4^2} = 5$
2. 3차원 벡터: (1, 2, 2) → 크기: $\sqrt{1^2 + 2^2 + 2^2} = 3$
---
## 5. 내적(Dot Product, 점곱)

1. 두 벡터 $\vec{a} = (a_1, a_2, ..., a_n)$, $\vec{b} = (b_1, b_2, ..., b_n)$의 내적: $a_1b_1 + a_2b_2 + ... + a_nb_n$
2. 내적의 결과는 스칼라(숫자)
3. 내적은 두 벡터가 이루는 각의 코사인값과 각 벡터의 크기를 곱한 것과 같음: $\vec{a} \cdot \vec{b} = |\vec{a}| |\vec{b}| \cos\theta$
4. 내적은 두 벡터의 방향 유사성, 투영 등을 나타낼 때 사용
---
## 6. 넘파이의 차원 (Numpy Array의 차원)

넘파이(NumPy)는 파이썬에서 수치 계산을 할 때 사용하는 대표적인 라이브러리로, 다양한 차원의 배열(array)을 다룰 수 있습니다. 배열의 차원은 데이터가 어떻게 구조화되어 있는지를 나타냅니다.

| 차원 | 명칭         | 구조 예시                        | 용어(수학) | 실제 예시                      |
|------|--------------|----------------------------------|------------|-------------------------------|
| 1차원 | 벡터         | [1, 2, 3, 4]                     | Vector     | 학생들의 시험 점수 목록         |
| 2차원 | 행렬         | [[1, 2, 3], [4, 5, 6]]           | Matrix     | 여러 학생의 여러 과목 점수      |
| 3차원 | 행렬의 집합  | [[[1, 2], [3, 4]], [[5, 6], [7, 8]]] | 3D Array   | 흑백 이미지 여러 장 데이터      |

> **정리:**
> - 1차원: [a, b, c, ...] (벡터)
> - 2차원: [[a, b], [c, d], ...] (행렬)
> - 3차원: [[[a, b], [c, d]], [[e, f], [g, h]], ...] (행렬의 집합)
---
## 6. 벡터, 매트릭스, 텐서의 차원과 관계 (Vector, Matrix, Tensor)

벡터, 매트릭스, 텐서는 데이터의 구조와 차원을 표현하는 기본 단위입니다. 차원이 높아질수록 더 복잡하고 많은 정보를 담을 수 있습니다.

### 6.1 벡터(Vector)
- 1차원 배열(리스트)로, 여러 개의 수(값)를 한 줄로 나열한 구조입니다.
- 예시: [1, 2, 3, 4] (학생들의 시험 점수 목록)

### 6.2 매트릭스(Matrix)
- 2차원 배열로, 여러 개의 벡터(행 또는 열)를 한데 모아 만든 표 형태의 구조입니다.
- 예시: [[1, 2, 3], [4, 5, 6]] (여러 학생의 여러 과목 점수)
- 즉, 벡터 여러 개를 세로/가로로 쌓으면 매트릭스가 됩니다.

### 6.3 텐서(Tensor)
- 3차원 이상(3D, 4D, ...)의 다차원 배열을 의미합니다.
- 예시: [[[1, 2], [3, 4]], [[5, 6], [7, 8]]] (흑백 이미지 여러 장 데이터)
- 즉, 매트릭스 여러 개를 쌓으면 텐서가 됩니다.
- 4차원 텐서: 컬러 이미지 여러 장(이미지, 채널, 행, 열 등)

| 차원 | 명칭/구조         | 예시 값         | 실제 데이터 예시                |
|------|-------------------|-----------------|---------------------------------|
| 0D   | 점(스칼라)        | 5               | 단일 값(온도, 키 등)            |
| 1D   | 벡터              | [2]             | 키, 몸무게 등 단일 특성 데이터  |
| 2D   | 매트릭스(행렬)    | [[1,2],[3,4]]   | 여러 학생의 여러 과목 점수      |
| 3D   | 텐서(3D 배열)     | [[[1,2],[3,4]], [[5,6],[7,8]]] | 흑백 이미지 여러 장 데이터      |
| 4D   | 텐서(4D 배열)     | (예시 생략)     | 컬러 이미지 여러 장(배치, 채널, 행, 열) |

> **정리:**
> - 벡터 여러 개를 모으면 매트릭스(2D 배열), 매트릭스 여러 개를 모으면 텐서(3D 이상 배열)가 됩니다.
> - 텐서는 딥러닝, 이미지 처리 등에서 대용량 데이터를 다룰 때 필수적으로 사용됩니다.
---
## 7. 벡터 공간(Vector Space)

벡터 공간은 벡터들이 모여 있는 집합으로, 벡터 연산(덧셈, 스칼라 곱 등)이 정의되어 있는 수학적 구조입니다. 벡터 공간의 모든 원소(점)는 벡터이며, 다음과 같은 조건을 만족해야 합니다.

### 7.1 벡터 공간의 조건
- 임의의 두 벡터의 덧셈 결과도 벡터 공간에 속한다 (덧셈에 대해 닫혀 있음)
- 임의의 벡터와 스칼라(숫자)의 곱도 벡터 공간에 속한다 (스칼라 곱에 대해 닫혀 있음)
- 0벡터(모든 성분이 0인 벡터)가 존재한다
- 벡터 덧셈, 스칼라 곱에 대해 결합법칙, 분배법칙 등이 성립한다

### 7.2 예시
- 2차원 평면상의 모든 벡터의 집합 (예: (x, y) 형태의 모든 벡터)
- 3차원 공간상의 모든 벡터의 집합 (예: (x, y, z) 형태의 모든 벡터)
- n차원 실수 벡터의 집합 ($\mathbb{R}^n$)

### 7.3 데이터 분석과 벡터 공간
- 데이터 분석에서는 여러 특성(피처)로 이루어진 데이터 한 줄(샘플)을 n차원 벡터로 보고, 전체 데이터셋을 n차원 벡터 공간으로 생각합니다.
- 벡터 공간 개념을 통해 데이터 간의 거리, 방향, 투영, 차원 축소 등 다양한 수학적 처리가 가능합니다.
---
## 8. 2개의 벡터의 합 (벡터 덧셈)

### 8.1 벡터 덧셈의 개념
두 벡터의 합은 각 성분별로 더하는 방식으로 계산합니다. 2차원 공간에서 벡터 A와 벡터 B의 합은 다음과 같습니다.

- 벡터 A = [A1, A2]
- 벡터 B = [B1, B2]
- 벡터 A + 벡터 B = [A1+B1, A2+B2]

### 8.2 도식적 예시
벡터 A와 벡터 B를 평면에 화살표로 그렸을 때, 벡터 A의 끝점에서 벡터 B를 시작하면 두 벡터의 합은 시작점에서 끝점까지의 벡터(대각선)로 나타낼 수 있습니다. 이를 '평행사변형 법칙'이라고도 합니다.

예시
- 벡터 A = [2, 1], 벡터 B = [1, 3]
- 벡터 A + 벡터 B = [2+1, 1+3] = [3, 4]

### 8.3 데이터 분석에서의 활용 예시
데이터 분석에서는 여러 특성(피처) 값을 벡터로 보고, 두 데이터의 합을 구할 때 벡터 덧셈을 사용합니다.

예시
- 고객1의 특성: [나이, 구매횟수] = [30, 5]
- 고객2의 특성: [40, 2]
- 두 고객의 특성 합: [30+40, 5+2] = [70, 7]
---
## 9. 나중에 배울 것

### 9.1 Word to Vector (단어의 벡터화)
- 단어를 컴퓨터가 이해할 수 있도록 수치 벡터로 변환하는 방법입니다. 대표적으로 Word2Vec, GloVe, FastText 등이 있습니다.
- 예를 들어, "king"이라는 단어가 [0.25, 0.13, ...]처럼 100차원, 200차원 등 고차원 벡터로 표현됩니다.
- 이렇게 벡터로 변환하면 단어 간의 의미적 유사성, 관계 등을 수치적으로 계산할 수 있습니다.

### 9.2 코사인 유사도 (Cosine Similarity)
- 두 벡터가 이루는 각의 코사인값을 이용해 유사도를 측정하는 방법입니다.
- 수식: \( \cos(\theta) = \frac{\vec{a} \cdot \vec{b}}{||\vec{a}|| \cdot ||\vec{b}||} \)
- 값의 범위는 -1 ~ 1이며, 1에 가까울수록 두 벡터(단어)가 비슷한 의미를 가집니다.
- 예시: "king"과 "queen"의 벡터가 비슷한 방향을 가진다면, 코사인 유사도 값이 1에 가까워집니다.

### 9.3 벡터 연산을 통한 단어 관계 추론
- 벡터의 덧셈/뺄셈을 통해 단어 간의 의미적 관계를 수치적으로 표현할 수 있습니다.
- 예시: king - man + woman = queen
  - 'king'에서 'man'의 의미를 빼고 'woman'을 더하면 'queen'과 가장 가까운 벡터가 나옵니다.
- 이런 방식으로 단어 간의 관계(예: 남자-여자, 나라-수도 등)를 유추할 수 있습니다.

### 9.4 벡터 데이터베이스 (Vector Database)
- 수많은 벡터(문서, 이미지, 단어 등)를 저장하고, 주어진 벡터와 가장 가까운 벡터(유사한 의미)를 빠르게 찾아주는 데이터베이스입니다.
- 예시: 이미지 검색, 문서 검색, 추천 시스템 등에서 활용됩니다.
- 원리: 모든 데이터를 벡터로 변환한 뒤, 코사인 유사도 등으로 가장 가까운 벡터를 찾아 결과로 반환합니다.

> **정리:**
> - 단어, 문서, 이미지 등 다양한 데이터를 벡터로 변환하면, 수치적으로 유사성/관계/검색이 가능해집니다.
> - 코사인 유사도와 벡터 연산을 통해 의미적 관계를 파악하고, 벡터 데이터베이스를 활용해 대규모 데이터에서 유사한 항목을 빠르게 찾을 수 있습니다.
---

## 10. 선형 결합, 선형 독립, 선형 종속

### 10.1 선형 결합 (Linear Combination)
- 여러 벡터에 각각 상수(스칼라)를 곱한 뒤 더하는 연산을 말합니다.
- 예시: 벡터 A, B가 있을 때, 2A + 3B와 같이 각 벡터에 상수를 곱해서 더할 수 있습니다.
- 수식: $c_1\vec{v}_1 + c_2\vec{v}_2 + ... + c_n\vec{v}_n$ (여기서 c는 상수, v는 벡터)
- 벡터에 2를 곱하면 크기가 2배로 늘어나고, -1을 곱하면 방향이 반대가 됩니다.

### 10.2 벡터 관계: 선형 독립 (Linear Independence)
- 여러 벡터 중에서, 어떤 벡터도 나머지 벡터들의 선형 결합으로 표현할 수 없다면 '선형 독립'이라고 합니다.
- 즉, 한 벡터가 다른 벡터들의 조합으로 만들어질 수 없을 때 선형 독립입니다.
- 예시: (1,0), (0,1)은 서로 선형 독립입니다. (2,3)은 (1,0), (0,1)로 표현할 수 있지만, (1,0), (0,1) 자체는 서로 조합해서 만들 수 없습니다.

### 10.3 벡터 관계: 선형 종속 (Linear Dependence)
- 여러 벡터 중에서, 어떤 벡터가 나머지 벡터들의 선형 결합으로 표현될 수 있다면 '선형 종속'입니다.
- 즉, 한 벡터가 다른 벡터들의 조합으로 만들어질 수 있을 때 선형 종속입니다.
- 예시: (2,4)는 (1,2)의 2배이므로, 두 벡터는 선형 종속입니다.

#### 정리 및 활용
- 선형 독립/종속 개념은 벡터 공간의 차원(기저 벡터 개수)과 관련이 깊으며, 데이터 분석, 머신러닝, 차원 축소(예: PCA) 등에서 매우 중요하게 사용됩니다.

---

## 11. mpg dataset

### mpg(miles per gallon, mpg) dataset 이란?:

자동차의 연비(miles per gallon, mpg)와 관련된 다양한 정보를 담고 있는 대표적인 예제 데이터셋입니다. 주로 통계, 데이터 분석, 머신러닝 입문에서 많이 사용됩니다.

### 주요 특징
- 각 행(row)은 한 대의 자동차 정보를 나타냅니다.
- 주요 컬럼(변수):
  - mpg: 연비 (갤런당 마일)
  - cylinders: 실린더 개수
  - displacement: 배기량
  - horsepower: 마력
  - weight: 차량 무게
  - acceleration: 가속력
  - model year: 연식
  - origin: 제조국(미국, 유럽, 일본 등)
  - name: 자동차 이름


### 예시 데이터 (표)

| 연비(mpg) | 실린더수 | 배기량(cc) | 마력 | 차량무게(kg) | 가속력(초) | 연식 | 제조국 | 자동차 이름 |
|-----|-----------|--------------|------|------------|----------|------|--------|----------------------|
| 18  | 8         | 307          | 130  | 3504       | 12.0     | 70   | 미국   | 쉐보레 셰벨         |
| 15  | 8         | 350          | 165  | 3693       | 11.5     | 70   | 미국   | 뷰익 스카이락 320    |
| 36  | 4         | 91           | 69   | 2130       | 14.7     | 82   | 일본   | 혼다 시빅           |
| 27  | 4         | 97           | 88   | 2130       | 14.5     | 70   | 유럽   | 폭스바겐 타입 3      |
| 24  | 4         | 113          | 95   | 2228       | 14.0     | 71   | 미국   | 포드 핀토           |


### 활용 예시 (상세)
- **연비와 변수 간의 관계 분석**: 무게, 배기량, 마력 등과 연비(mpg) 사이의 상관관계를 시각화(산점도, 상관계수 등)로 분석할 수 있습니다.
- **회귀분석(선형/비선형)**: 연비를 예측하는 회귀모델(선형회귀, 다항회귀 등)을 만들고, 변수의 영향력을 해석할 수 있습니다.
- **분류(Classification)**: 예를 들어, 연비가 높은 차/낮은 차로 구분하는 분류 문제에 활용할 수 있습니다.
- **결측치 처리**: horsepower 등 일부 변수에 결측치가 포함되어 있어, 결측치 처리(평균 대체, 삭제 등) 실습에 적합합니다.
- **데이터 전처리**: 범주형 변수(제조국, 이름 등) 인코딩, 이상치 탐지, 정규화 등 다양한 전처리 실습이 가능합니다.
- **데이터 시각화**: 히스토그램, 박스플롯, 페어플롯 등 다양한 시각화 기법을 적용해 볼 수 있습니다.
- **차원 축소(PCA, 주성분분석)**: 여러 변수(무게, 배기량, 마력 등)를 소수의 주성분으로 축소하여 데이터의 구조를 시각화하거나, 분석의 효율을 높일 수 있습니다.
  - 예시: 7~8개의 수치형 변수를 2~3개의 주성분으로 줄여 2D/3D 산점도로 시각화
- **클러스터링(군집분석)**: 비슷한 특성을 가진 자동차끼리 그룹화(군집화)하여, 시장 세분화나 특성 분석에 활용할 수 있습니다.
- **머신러닝 실습**: 지도학습(회귀, 분류), 비지도학습(PCA, 클러스터링) 등 다양한 머신러닝 알고리즘 실습에 적합합니다.

> **참고:**
> - 파이썬의 seaborn, statsmodels, R 등에서 내장 데이터셋으로 쉽게 불러올 수 있습니다.

---

## 12. 초평면
### 초평면(Hyperplane)이란?

- 초평면은 n차원 공간에서 (n-1)차원의 평면을 의미합니다.
  - 2차원 공간의 초평면: 1차원 직선
  - 3차원 공간의 초평면: 2차원 평면
  - 4차원 이상 공간의 초평면: 3차원 이상의 평면(직관적으로 그릴 수 없음)

- 수식으로는 다음과 같이 표현됩니다.
  - $w_1x_1 + w_2x_2 + ... + w_nx_n + b = 0$
  - 여기서 w는 각 축의 계수(가중치), x는 변수, b는 절편(바이어스)

### 예시
- 2차원: x + y = 1 (직선)
- 3차원: x + y + z = 1 (평면)

### 데이터 분석/머신러닝에서의 활용
- 초평면은 데이터를 분리하거나(분류), 투영하거나(차원 축소)할 때 자주 등장합니다.
- 대표적으로 SVM(서포트 벡터 머신)에서 데이터를 두 그룹으로 나누는 경계선(결정 경계)이 바로 초평면입니다.
- PCA(주성분분석)에서도 데이터의 분산이 가장 큰 방향(주성분)을 기준으로 초평면에 데이터를 "투영"해 차원을 축소합니다.
  - 여기서 "투영"이란, 고차원 공간에 있는 데이터를 초평면(낮은 차원 공간)에 그림자처럼 내려놓는 것(=비추어 옮기는 것)입니다.
  - 예를 들어, 3차원 공간의 점들을 2차원 평면(초평면)에 가장 잘 펼쳐지도록 눌러서 옮기는 것과 같습니다.
  - 이렇게 하면 데이터의 주요 특성(정보)을 최대한 보존하면서 차원을 줄일 수 있습니다.
  - 투영은 데이터의 구조를 시각화하거나, 분석을 더 쉽게 할 때 매우 유용하게 사용됩니다.

**정리:**
- 초평면은 고차원 공간에서 데이터를 나누거나 투영하는 데 핵심적인 역할을 하며, 머신러닝/통계/수학 등 다양한 분야에서 매우 중요하게 사용됩니다.

---

## 13. 내적(Dot Product)와 정사영(Projection) 정리

### 13.1 내적(Dot Product)이란?
1. 두 벡터의 내적은 두 벡터가 얼마나 같은 방향을 가리키는지(방향 유사도)와 크기를 동시에 측정하는 연산입니다.
2. 내적의 결과는 하나의 실수 값(스칼라)입니다.
3. 수식: $\vec{a} \cdot \vec{b} = |\vec{a}| |\vec{b}| \cos(\theta)$
   - $|\vec{a}|$, $|\vec{b}|$는 각 벡터의 크기, $\theta$는 두 벡터가 이루는 각

#### 13.1.1 각도별 내적의 의미
| 각도 $\theta$ | $\cos(\theta)$ 값 | 내적의 의미 |
|:---:|:---:|:---|
| $0°$ | $1$ | 완전히 같은 방향 (최대값) |
| $60°$ | $0.5$ | 비슷한 방향 |
| $90°$ | $0$ | 완전히 수직 (내적=0, 영향 없음) |
| $180°$ | $-1$ | 완전히 반대 방향 (최소값) |

#### 13.1.2 내적의 해석과 특징
1. 코사인 세타($\cos(\theta)$)가 0이면 두 벡터가 서로 90도(수직)임을 의미하며, 이때 내적의 결과도 0이 됩니다.
2. 내적의 값이 클수록 두 벡터가 유사한(같은 방향) 벡터임을 의미합니다.
3. 내적의 값이 음수이면 두 벡터의 방향이 서로 반대임을 의미합니다.
4. 내적의 크기를 통해 두 벡터가 서로 어떤 관계(방향, 유사성, 반대 등)를 갖고 있는지 알 수 있습니다.

#### 13.1.3 코사인(Cosine) 함수와 내적
1. 코사인 함수는 각의 크기에 따라 두 벡터의 방향 관계를 수치로 표현합니다.
2. $\cos(\theta) = \frac{\text{밑변}}{\text{빗변}}$
3. 내적 계산, 벡터의 유사도, 데이터 분석 등 다양한 분야에서 매우 중요하게 사용됩니다.

---

### 13.2 정사영(Projection)이란?
1. 한 벡터를 다른 벡터 위에 "그림자처럼" 내리는 연산을 정사영(투영)이라고 합니다.
2. 벡터 $\vec{A}$를 벡터 $\vec{B}$ 위로 수선의 발을 내리면, $\vec{B}$ 위에 $\vec{A}$의 그림자가 생깁니다.
3. 이 그림자의 길이(=정사영의 크기)는 $|\vec{A}| \cos(\theta)$로 계산할 수 있습니다.
4. 내적은 바로 이 정사영의 크기에 벡터 $\vec{B}$의 크기를 곱한 것과 같습니다.
   - 즉, $\vec{A} \cdot \vec{B} = |\vec{A}| |\vec{B}| \cos(\theta)$
5. 내적은 수식으로 직접 계산할 수도 있고, 정사영의 개념으로도 이해할 수 있습니다.

#### 13.2.1 정리
1. 내적은 두 벡터의 방향과 크기 유사도를 동시에 측정하는 연산이며, 정사영은 한 벡터를 다른 벡터 위에 투영(그림자)하는 과정입니다.
2. 내적의 결과는 항상 하나의 실수 값(스칼라)입니다.

---

### 13.3 내적을 계산하는 방법
내적은 두 가지 방식으로 계산할 수 있습니다.

1. **기하학적 정의(각도와 크기 이용):**
   - $\vec{A} \cdot \vec{B} = |\vec{A}| \times |\vec{B}| \times \cos(\theta)$
   - 두 벡터의 크기와, 두 벡터가 이루는 각($\theta$)의 코사인값을 곱합니다.
   - 이 방식은 벡터의 방향과 크기, 각도에 대한 직관적 이해에 유용합니다.

2. **성분(좌표) 기반 계산:**
   - 각 성분별 곱의 합으로 계산합니다. (머신러닝, 데이터 분석에서 주로 사용)
   - 예시 (2차원):
     - $\vec{A} = [A_1, A_2]$, $\vec{B} = [B_1, B_2]$
     - $\vec{A} \cdot \vec{B} = A_1 \times B_1 + A_2 \times B_2$
   - 예시 (3차원):
     - $\vec{A} = [A_1, A_2, A_3]$, $\vec{B} = [B_1, B_2, B_3]$
     - $\vec{A} \cdot \vec{B} = A_1 \times B_1 + A_2 \times B_2 + A_3 \times B_3$
   - 이 방식은 실제 데이터(표, 배열 등)에서 벡터 연산을 빠르게 계산할 때 매우 유용합니다.

---

### 14. 행렬(Matrix)과 딥러닝 모델의 학습 과정

#### 14.1 행렬의 개념과 연산
1. 여러 개의 열 벡터(또는 행 벡터)를 모으면 2차원 배열인 행렬(Matrix)이 됩니다.
   - 예시: 두 개의 열 벡터 $\begin{bmatrix}1 \\ 2 \\ 3\end{bmatrix}$, $\begin{bmatrix}4 \\ 5 \\ 6\end{bmatrix}$를 옆으로 붙이면 $\begin{bmatrix}1 & 4 \\ 2 & 5 \\ 3 & 6\end{bmatrix}$와 같은 행렬이 됩니다.
2. 행렬은 벡터를 여러 개 모아놓은 표 형태의 데이터 구조로, 다양한 연산(특히 행렬 곱)을 할 수 있습니다.
3. 행렬 곱은 데이터 변환, 신경망의 연산, 이미지 처리 등에서 매우 중요하게 사용됩니다.
   - 예시: 입력 벡터(특성) $[x_1, x_2, x_3]$와 가중치 행렬 $\begin{bmatrix}w_{11} & w_{12} \\ w_{21} & w_{22} \\ w_{31} & w_{32}\end{bmatrix}$를 곱하면, 두 개의 출력값(예측값)이 나옵니다.

#### 14.2 딥러닝/머신러닝에서의 행렬 활용
1. 신경망(Neural Network)에서는 여러 입력값(숫자, 확률 등)이 한 번에 들어오고, 이 입력들은 행렬로 표현됩니다.
2. 각 입력값(예: 온도, 풍속, 습도 등)에 가중치(Weight, W1, W2, W3 등)를 곱해 더하는 연산이 반복됩니다.
   - 예시: 입력 $[25, 5, 10]$ × 가중치 $[0.2, -0.1, 0.5]$ → $25\times0.2 + 5\times(-0.1) + 10\times0.5 = 5 + (-0.5) + 5 = 9.5$
3. 이 과정은 행렬 곱과 내적(dot product) 연산으로 표현할 수 있습니다.
   - 여러 입력 샘플이 있을 때는 입력 행렬 × 가중치 벡터/행렬로 한 번에 여러 예측값을 계산할 수 있습니다.

#### 14.3 지도학습(감성분석 예시)
1. 지도학습(Supervised Learning)은 입력(문제지)과 정답(답지, 레이블)을 함께 주고 학습하는 방식입니다.
2. 예시 데이터:
   - (예시1) 온도 25도, 풍속 5, 습도 10 → 기분이 좋다(1)
   - (예시2) 온도 37도, 풍속 1, 습도 70 → 기분이 나쁘다(0)
   - (예시3) 온도 10도, 풍속 8, 습도 30 → 기분이 좋다(1)
   - (예시4) 온도 30도, 풍속 2, 습도 80 → 기분이 나쁘다(0)
3. 여러 입력(특성)과 정답(라벨) 데이터를 표로 정리하면 다음과 같습니다.

| 온도 | 풍속 | 습도 | 기분(정답) |
|------|------|------|------------|
| 25   | 5    | 10   | 1 (좋다)   |
| 37   | 1    | 70   | 0 (나쁘다) |
| 10   | 8    | 30   | 1 (좋다)   |
| 30   | 2    | 80   | 0 (나쁘다) |

4. 위 예시는 감성 분석(이진 분류) 문제로, 입력(온도, 풍속, 습도) → 출력(기분 좋다/나쁘다)로 예측하는 모델을 만듭니다.

#### 14.4 딥러닝 모델의 학습 과정 요약
1. 입력 데이터(특성들)에 임의의 가중치(난수)를 곱해 내적을 계산합니다.
   - 예시: $[25, 5, 10] \cdot [0.2, -0.1, 0.5] = 9.5$
2. 내적 결과(예측값)를 활성화 함수(예: 시그모이드, 소프트맥스 등)에 통과시켜 최종 출력값(확률, 분류 등)을 만듭니다.
3. 예측값과 실제값(정답)의 차이(오차, Loss)를 계산합니다.
4. 오차를 줄이기 위해 가중치(Weight)를 조금씩 조정(역전파, Gradient Descent)합니다.
5. 이 과정을 여러 번 반복(에포크, Epoch)하면서 모델이 점점 더 정확하게 예측하도록 학습합니다.
6. 반복 학습을 통해 입력과 출력 간의 패턴을 찾아내고, 미지의 데이터에 대해서도 예측이 가능해집니다.

#### 14.5 추가 설명 및 주의사항
1. 모델의 성능은 정확도(Accuracy), 손실(Loss) 등으로 평가하며, 80~90% 이상의 정확도를 목표로 합니다.
2. ImageNet: 이미지 분류 대회로 유명했으나, 현재는 AI가 사람보다 이미지를 더 잘 분류하는 수준에 도달해 경연이 중단됨.
3. 자연어 처리(NLP) 분야도 AI가 빠르게 발전 중이나, GPT 등 생성형 AI는 계산/추론 결과에 오류(할루시네이션)가 있을 수 있으므로 항상 검증이 필요합니다.
4. 딥러닝 모델은 입력 데이터의 스케일(정규화), 데이터의 다양성, 오버피팅 방지 등도 매우 중요합니다.

> **정리:**
> - 행렬은 여러 벡터(특성)를 모아놓은 2차원 데이터 구조로, 딥러닝/머신러닝에서 입력, 가중치, 출력 등 다양한 연산에 필수적으로 사용됩니다.
> - 행렬 곱과 내적은 신경망의 핵심 연산이며, 반복 학습을 통해 모델이 데이터의 패턴을 학습하고 예측 성능을 높입니다.
> - 실제 예측 과정에서는 활성화 함수, 손실 함수, 역전파 등 다양한 수학적/컴퓨터적 기법이 함께 사용됩니다.