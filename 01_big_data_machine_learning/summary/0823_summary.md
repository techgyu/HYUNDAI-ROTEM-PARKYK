# 08/23 수업 내용 정리

---

## 1. 선형 회귀
### 1.1 단순 선형 회귀
| 내용                           | 설명                                                                 |
|-------------------------------|---------------------------------------------------------------------|
| **기본 매커니즘**              | 데이터 산포를 바탕으로 예측력이 가장 뛰어난 직선<br>($wx + b = f(x)$)을 그려내어 새로운 값 예측 |
| **계산 방법**                  | 학습으로 $w$(기울기), $b$(절편)을 구하는 것이 아니라<br>최소 제곱법(ols, linregress, LinearRegression 등)으로 상대적으로 단순하게 계산 |
| **추후 학습 방법**             | 나중에는 미분을 이용한 학습(경사하강법 등)으로 모델을 만드는 방법도 배움 |
| **결과 해석**                  | 최소 제곱법 함수(statsmodels.formula.api.ols, sklearn.linear_model.LinearRegression, scipy.stats.linregress)는 결과로 표(요약 통계)를 제공하며, 이 표에서 정보를 읽을 줄 알아야 함 |

### 1.2.1 선형 회귀 결과 표 해석
                            OLS Regression Results
==============================================================================   
Dep. Variable:                  만족도   R-squared:                       0.588
Model:                            OLS   Adj. R-squared:                  0.586   
Method:                 Least Squares   F-statistic:                     374.0   
Date:                Mon, 25 Aug 2025   Prob (F-statistic):           2.24e-52   
Time:                        09:54:35   Log-Likelihood:                -207.44   
No. Observations:                 264   AIC:                             418.9   
Df Residuals:                     262   BIC:                             426.0   
Df Model:                           1
Covariance Type:            nonrobust
==============================================================================

| 항목                        | 설명                                                                                   |
|-----------------------------|----------------------------------------------------------------------------------------|
| **Dep. Variable**           | 종속 변수(예측하고자 하는 값), 여기서는 '만족도'
| **R-squared**               | 결정계수, 모델이 데이터를 얼마나 잘 설명하는지(0.588 → 약 58.8% 설명력)
| **Adj. R-squared**          | 수정 결정계수, 변수 개수에 따라 보정된 설명력(0.586)
| **Model**                   | 사용한 회귀 모델(OLS: 최소제곱법)
| **Method**                  | 회귀분석에 사용된 방법(Least Squares: 최소제곱법)
| **F-statistic**             | 모델 전체의 유의성 검정 통계량(374.0)
| **Prob (F-statistic)**      | F-통계량의 p-value(2.24e-52), 모델이 통계적으로 유의미한지 판단(아래 t 값의 제곱)
| **Date/Time**               | 분석 실행 날짜와 시간
| **Log-Likelihood**          | 로그 우도, 모델의 적합도(값이 클수록 적합도가 높음)
| **No. Observations**        | 데이터 샘플 개수(264개)
| **AIC/BIC**                 | 모델 평가 지표(Akaike/Bayesian 정보 기준), 값이 낮을수록 좋은 모델
| **Df Residuals**            | 잔차(오차) 자유도(262)
| **Df Model**                | 모델에 사용된 독립 변수 개수(1개)
| **Covariance Type**         | 공분산 추정 방식(nonrobust: 일반적 방식)


#### 1.2.2 회귀계수 및 통계량
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]   
------------------------------------------------------------------------------   
Intercept      0.7789      0.124      6.273      0.000       0.534       1.023   
적절성            0.7393      0.038     19.340      0.000       0.664       0.815

| 항목             | 값        | 설명
|------------------|-----------|--------------------------------------------------------------|
| Intercept           | 0.7789                        | 절편(입력값이 0일 때 만족도 예측값)
| Intercept std err   | 0.124                         | 절편의 표준오차(신뢰도)
| Intercept t         | 6.273                         | 절편의 t-통계량(유의성 검정)
| Intercept P>|t|     | 0.000                         | 절편의 p-value(통계적 유의성)
| Intercept [0.025, 0.975] | 0.534 ~ 1.023            | 절편의 95% 신뢰구간
| 적절성 coef         | 0.7393                        | 독립 변수 '적절성'의 회귀계수(영향력)
| 적절성 std err      | 0.038                         | 적절성의 표준오차(신뢰도)
| 적절성 t            | 19.340                        | 적절성의 t-통계량(유의성 검정)
| 적절성 P>|t|        | 0.000                         | 적절성의 p-value(통계적 유의성)
| 적절성 [0.025, 0.975] | 0.664 ~ 0.815               | 적절성의 95% 신뢰구간


#### 1.2.3 잔차 및 기타 지표
==============================================================================   
Omnibus:                       11.674   Durbin-Watson:                   2.185   
Prob(Omnibus):                  0.003   Jarque-Bera (JB):               16.003   
Skew:                          -0.328   Prob(JB):                     0.000335   
Kurtosis:                       4.012   Cond. No.                         13.4
==============================================================================

| 항목             | 값        | 설명
|------------------|-----------|--------------------------------------------------------------|
| Omnibus          | 11.674    | 잔차의 정규성 검정 통계량
| Prob(Omnibus)    | 0.003     | Omnibus 검정의 p-value
| Durbin-Watson    | 2.185     | 잔차의 자기상관 검정(2에 가까우면 잔차가 독립적임)
| Jarque-Bera (JB) | 16.003    | 잔차의 정규성 검정 통계량
| Prob(JB)         | 0.000335  | Jarque-Bera 검정의 p-value
| Skew             | -0.328    | 잔차의 왜도(비대칭성)
| Kurtosis         | 4.012     | 잔차의 첨도(뾰족함)
| Cond. No.        | 13.4      | 다중공선성 지표(높으면 변수 간 중복 영향 가능성)



### 1.3 독립 변수가 여러 개인 선형 회귀
| 내용             | 설명                                                                                                 |
|------------------|-----------------------------------------------------------------------------------------------------|
| **기본 매커니즘** | 여러 개의 독립 변수($x_1, x_2, x_3, x_4$ 등)에 각각의 가중치($w_1, w_2, w_3, w_4$)를 곱해 모두 더하고, 절편($b$)을 더해서 종속 변수($y$)를 예측함.<br>즉, 여러 요인이 결과에 동시에 영향을 주는 경우에 사용. <br>공식: $y = w_1x_1 + w_2x_2 + w_3x_3 + w_4x_4 + b$ |
| **계산 방법**    | 최소 제곱법(OLS, LinearRegression 등)을 사용해 각 변수의 회귀계수($w_1, w_2, ...$)와 절편($b$)을 계산.<br>데이터가 많아질수록 변수 간의 상관관계와 각 변수의 영향력을 더 정확하게 파악할 수 있음. |
| **결과 해석**    | 각 독립 변수의 회귀계수는 해당 변수의 영향력(기여도)을 의미.<br>회귀 결과 표에서 각 변수의 계수, p-value, 결정계수(R²) 등을 확인하여 모델의 적합도와 변수의 유의성을 해석함.<br>변수가 많을수록 다중공선성(변수 간 중복 영향)도 함께 고려해야 함. |
| **추가 사항**    | 다중 선형 회귀는 변수 선택, 변수 변환, 정규화 등 추가적인 데이터 전처리와 모델 평가가 중요함.<br>실제 현업에서는 변수 간의 관계를 시각화하거나, 변수 선택 기법을 활용해 모델의 성능을 높임. |


## 기타 전달사항
- 금요일 끝날 때쯤 팀 편성 및 자리 변경, 다음 주 월요일부터 바뀐 자리에서 수업 진행
- 파이널 프로젝트 진행할 때는 오전에 수업하고 오후에는 프로젝트 시간을 비워줍니다.